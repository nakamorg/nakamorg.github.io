"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"2025/managing-resume-and-auto-generation","metadata":{"permalink":"/blog/2025/managing-resume-and-auto-generation","source":"@site/../blogs/2025-04-10-managing-resume-and-auto-generation.md","title":"2025-04-10 Managing Resume And Auto-generation","description":"I recently updated my resume, which involved the usual process:","date":"2025-04-10T00:00:00.000Z","tags":[{"inline":true,"label":"2025-04","permalink":"/blog/tags/2025-04"},{"inline":true,"label":"2025","permalink":"/blog/tags/2025"},{"inline":true,"label":"md2pdf","permalink":"/blog/tags/md-2-pdf"},{"inline":true,"label":"resume","permalink":"/blog/tags/resume"},{"inline":true,"label":"automation","permalink":"/blog/tags/automation"}],"readingTime":3.605,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"2025-04-10 Managing Resume And Auto-generation","slug":"2025/managing-resume-and-auto-generation","tags":["2025-04","2025","md2pdf","resume","automation"],"hide_table_of_contents":false},"unlisted":false,"nextItem":{"title":"Umesh Poswal Resume","permalink":"/blog/resume-umesh-poswal"}},"content":"I recently updated my resume, which involved the usual process:\\n- Searching for achievements to add (I record these as they happen in Notion and a GitHub repo)\\n- Locating the previous Google Doc containing my resume\\n- Making in-place updates to that doc and spending a frustrating amount of time getting the indentation and spacing right in Google Docs\x3c!-- truncate --\x3e\\n- Downloading the document as a PDF and uploading it to Google Drive (I couldn\'t find a \\"Save as PDF\\" option in Google Docs to automatically save it to Drive)\\n\\nThis process requires a significant amount of data gathering and copying/pasting. I had the idea of managing my resume as a blog post on this blog, which is entirely written in Markdown. This would allow me to have everything in Markdown, tracked by Git, and stored on GitHub. I thought of storing everything solely on GitHub (instead of Notion and Google Docs) as Markdown and writing a Markdown processor to convert it to PDF (since we\'ll likely need PDFs for the foreseeable future).\\n\\nFortunately, I don\'t use much styling in my resume; it\'s mostly just simple text with bunch of heading, list items and a few horizontal lines. Since my use case wasn\'t that unique, I searched for existing tools, and `pandoc` stood out.\\n\\nPandoc seems complex to work with, especially with PDFs, and requires several extensions. This was initially a major barrier to entry, so I looked for simpler, purpose-built tools (Markdown to PDF only, as Pandoc is very general-purpose). However, none of them worked quite right or still required installing LaTeX or other extensions. In the end, I settled on Pandoc but didn\'t want to clutter my host with unnecessary extensions or libraries, so I decided to use a Docker-based setup.\\n\\n## The Setup\\nAt the time of writing, the `pandoc/latex:3` Docker image didn\'t have an ARM version available, but `pandoc/latex:3-ubuntu` did, so I opted for that one. Pandoc uses LaTeX by default for PDF conversion, and while you can pass in various parameters to control the styling, elements like headings, list items, etc., are more easily managed using HTML and CSS. And since I had a little experience with HTML and CSS (should add these to resume as well), I decided to use Pandoc\'s HTML processor for generating the PDF.\\n\\n### The dockerfile\\n```Dockerfile\\nFROM pandoc/latex:3-ubuntu\\n\\nWORKDIR /data\\n\\n# required by html processor\\nRUN apt update && apt upgrade -y && apt install -y weasyprint\\n\\nENTRYPOINT [\\"pandoc\\"]\\n\\n```\\n\\nAs I mentioned, my resume had a bunch of headings, list items, paragraphs, and some horizontal lines (everything written as Markdown text). I needed to control the styling of all these elements, and the output PDF needed to be formatted for A4 size paper. So, the CSS styling I ended up with was something like:\\n\\n### The css file\\n```css\\n/* Base styles */\\n@page {\\n    size: A4;\\n    margin: 0.35in 0.35in 0.35in 0.45in;\\n}\\n\\nbody {\\n    width: 100%;\\n    margin: 0;\\n    padding: 0;\\n    font-family: Arial, sans-serif;\\n    line-height: 1.15;\\n    font-size: 10pt;\\n}\\n\\n/* Headings */\\nh1 {\\n    margin: 0.2em 0 0.2em 0;\\n    font-size: 16pt;\\n}\\n\\nh2 {\\n    margin: 0.8em 0 0.2em 0;\\n    font-size: 14pt;\\n}\\n\\nli {\\n    margin: 0.15em 0;\\n    text-align: justify;\\n}\\n\\nhr {\\n    border: none;\\n    height: 1px;\\n    background-color: #ddd;\\n    margin: 0.2em 0;\\n}\\n\\n```\\n\\nIt all looked familiar, or at least was pretty easy to pick up, thanks to our AI lords.\\n\\nAnd that was pretty much it. Now, assuming you have these files in a directory structured as follows:\\n\\n```txt\\n\u251c\u2500\u2500 resume.md    # File containing your resume\\n\u251c\u2500\u2500 resume.css   # CSS styling for the PDF output\\n\u2514\u2500\u2500 Dockerfile   # Docker configuration file\\n```\\n\\nYou can (or at least I did) generate that nice looking pdf with\\n```sh\\ndocker build -t pandoc-md2pdf -f Dockerfile .\\ndocker run --rm -v \\"$(pwd):/data\\" pandoc-md2pdf -t html -c ./resume.css ./resume.md -o ./resume.pdf\\n```\\n\\n## The Future Ahead\\nWhile I may need to make some minor adjustments to the styling, I\'m highly confident that this new workflow will replace my previous process involving Google Docs. Moving forward, I can simply update the markdown file with any new achievements I want to showcase on my resume. Then, by running that abocve `docker run` command, I\'ll have a nice-looking PDF generated automatically. And, of course, my resume will be updated on this blog as well."},{"id":"resume-umesh-poswal","metadata":{"permalink":"/blog/resume-umesh-poswal","source":"@site/../blogs/2025-04-09-resume.md","title":"Umesh Poswal Resume","description":"Senior Platform Engineer, SmartNews | Tokyo, Japan","date":"2025-04-09T00:00:00.000Z","tags":[{"inline":true,"label":"2025-04","permalink":"/blog/tags/2025-04"},{"inline":true,"label":"2025","permalink":"/blog/tags/2025"},{"inline":true,"label":"resume","permalink":"/blog/tags/resume"}],"readingTime":3.275,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Umesh Poswal Resume","slug":"resume-umesh-poswal","tags":["2025-04","2025","resume"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"2025-04-10 Managing Resume And Auto-generation","permalink":"/blog/2025/managing-resume-and-auto-generation"},"nextItem":{"title":"2024-10-17 Docker On Macs","permalink":"/blog/2024/docker-on-macs"}},"content":"## Senior Platform Engineer, SmartNews | Tokyo, Japan\\nEmail: [nakamume@gmail.com](mailto:nakamume@gmail.com) | LinkedIn: [linkedin.com/in/nakamume](https://linkedin.com/in/nakamume) | Website: [www.nakam.org](https://www.nakam.org)\\n\\n## PROFESSIONAL SUMMARY\\n<hr />\\nSenior Platform Engineer with 7+ years of experience managing large-scale cloud infrastructure (20+ Kubernetes clusters, 2K+ nodes) on AWS. Expertise in platform engineering, cloud-native architecture, and leading critical infrastructure initiatives focusing on stability, security, and cost optimization.\x3c!-- truncate --\x3e\\n\\n## TECHNICAL SKILLS\\n<hr />\\n<table>\\n<tr><td><strong>Infrastructure & Cloud</strong></td><td>AWS, Kubernetes, Terraform, Crossplane, CircleCI, ArgoCD, Packer, Helm</td></tr>\\n<tr><td><strong>Observability</strong></td><td>OpenTelemetry, Honeycomb, Datadog, Prometheus, Grafana</td></tr>\\n<tr><td><strong>Languages</strong></td><td>Go, Python, Bash, Java, Jsonnet</td></tr>\\n<tr><td><strong>Data & Messaging</strong></td><td>PostgreSQL, Redis, DynamoDB, Kafka</td></tr>\\n</table>\\n\\n## EXPERIENCE\\n<hr />\\n\\n### Senior Platform Engineer | SmartNews Tokyo (July 2022 - Present)\\n- Led upgrade of **20+ EKS clusters** (**2K+ nodes**) across 3 Kubernetes versions by implementing automated EC2 AMI pipeline with **Packer**/**CircleCI**, coordinating API deprecation migrations with teams, and creating rollback strategies; resulted in zero-downtime upgrade and **$60K/year** savings in compute costs\\n- Scaled observability platform to handle **10B+ daily events** by designing **Kafka**-based buffering system with **otel-collector** and **Refinery**; implemented adaptive autoscaling using historical load patterns and queue metrics, reducing compute costs by **50%** and eliminating processing failures during peak loads\\n- Led multiple cross-team initiatives to optimize cloud infrastructure costs by **$100K/year** through systematic identification and cleanup of unused resources; developed automated policies for EBS snapshot lifecycle, load balancer pruning, and container image retention across **300+ projects**\\n- Enhanced security compliance for **ITGC/ISMS** by architecting strict production/non-production separation; migrated **300+ projects** from static credentials to **OIDC** tokens in **CircleCI**, implemented **IAM** role boundaries, and leveraged **VPC flow logs** for cross-environment access detection\\n- Modernized infrastructure management by implementing **Crossplane**-based provisioning for **100+ systems**; replaced ad-hoc **Terraform** workflows with continuous reconciliation, managing permissions for developers, pipelines and service-accounts across AWS and Kubernetes resources\\n- Led **dual-stack networking** migration initiative by implementing IPv6 support across VPCs with zero downtime; modified AWS load balancer and DNS controllers for **IPv6/AAAA** record support, created migration playbooks and delegated migration tasks, resulting in reduced IPv4 costs and improved network scalability\\n- Improved incident response by establishing and following structured protocols for critical production issues; successfully resolved IPv6 routing failures in ad systems, CI/CD pipeline outages, cross-cluster DNS conflicts, and container registry throttling with minimal business impact\\n\\n### Platform Engineer | SmartNews Tokyo (Dec 2021 - June 2022)\\n- Enhanced security by replacing legacy **bastion EC2** instance using shared SSH key with ephemeral tunnel instances; implemented CLI tool using **AWS EC2 Connect**, **AWS IAM**-based authentication and **SSH ProxyCommand**, reducing security risks while maintaining familiar SSH workflows for **200+ developers**\\n- Designed and implemented standardized **E2E** testing framework for **Kubernetes** controllers; built reusable components for infrastructure testing (pod creation, DNS resolution, load balancer provisioning), integrated automated metrics collection and alerting into the framework resulting in reduced **MTTD** and **MTTR**\\n\\n### Software Engineer - 2 | Telus International (Nov 2019 - Nov 2021)\\n- Led **17-person** distributed team through infrastructure split during company separation; designed and executed zero-downtime migration strategy for **10+ services**, maintaining **99.9% uptime** during transition with minimal impact to developer productivity\\n- Improved CI/CD reliability by implementing Jenkins **pipelines-as-code** using **Jenkins DSL** plugin, replacing manual pipeline management in Jenkins UI; migrated **50+ pipelines**, implemented **Git**-based version control with PR reviews, enabling automated rollbacks and disaster recovery\\n\\n### Software Engineer | Works Applications Tokyo (Oct 2017 - Oct 2019)\\n- Reduced operational toil by developing **ChatOps** automation platform; integrated with internal systems using custom APIs, automated common workflows for attendance tracking and ticket management serving **100+ employees**\\n- Decreased cloud costs by **$36K/year** through automated environment schedule management; built scheduler system with custom policies for **10+ development** environments, implemented usage analytics and shutdown procedures\\n\\n## EDUCATION\\n<hr />\\nBTech in Computer Science | Indian Institute of Technology, Bhubaneswar | GPA: 8.6/10 (2014-2017)\\n\\n## CERTIFICATES\\n<hr />\\nAWS Solutions Architect Professional, CKA - Certified Kubernetes Administrator, AWS SysOps Administrator, Azure Administrator Associate\\nVerifiable at: [credly.com/users/umesh-poswal/badges](https://credly.com/users/umesh-poswal/badges) (though most of these might be expired by now)"},{"id":"2024/docker-on-macs","metadata":{"permalink":"/blog/2024/docker-on-macs","source":"@site/../blogs/2024-10-17-docker-on-macs.md","title":"2024-10-17 Docker On Macs","description":"Docker has been pushing more and more for Docker Desktop, and most of their documentation is now centered around Docker Desktop. Good for them. We all need bread on the table. They are also very committed to keeping the Docker Engine and CLI open source. Good for us. I think it\'s a win-win. This post is about setting up Docker on Macs without Docker Desktop. It should integrate as if you are running Docker natively on your Mac and support multi-arch images (still lots of AMD servers out there).","date":"2024-10-17T00:00:00.000Z","tags":[{"inline":true,"label":"2024-10","permalink":"/blog/tags/2024-10"},{"inline":true,"label":"2024","permalink":"/blog/tags/2024"},{"inline":true,"label":"docker","permalink":"/blog/tags/docker"},{"inline":true,"label":"macbook","permalink":"/blog/tags/macbook"},{"inline":true,"label":"lima","permalink":"/blog/tags/lima"},{"inline":true,"label":"multiarch","permalink":"/blog/tags/multiarch"}],"readingTime":5.89,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"2024-10-17 Docker On Macs","slug":"2024/docker-on-macs","tags":["2024-10","2024","docker","macbook","lima","multiarch"]},"unlisted":false,"prevItem":{"title":"Umesh Poswal Resume","permalink":"/blog/resume-umesh-poswal"},"nextItem":{"title":"2024-10-09 The Internet Today","permalink":"/blog/2024/the-internet-today"}},"content":"Docker has been pushing more and more for Docker Desktop, and most of their documentation is now centered around Docker Desktop. Good for them. We all need bread on the table.\x3c!-- truncate --\x3e They are also very committed to keeping the Docker Engine and CLI open source. Good for us. I think it\'s a win-win. This post is about setting up Docker on Macs without Docker Desktop. It should integrate as if you are running Docker natively on your Mac and support multi-arch images (still lots of AMD servers out there).\\n\\nOkay, so let\'s keep it simple without further blabbering. We are going to use [Lima](https://github.com/lima-vm/lima) to create a Linux VM with Docker and install Docker CLI on the MacBook. The CLI will connect to the Docker daemon running on the Linux VM transparently.\\n\\n## Instllation\\n```bash\\nbrew install lima\\nbrew install --formula docker # docker cli\\nbrew install docker-buildx # follow post installation instructions\\n```\\n\\n## Create a Lima instance\\nUse following for the VM template.\\n```yaml\\n# ===================================================================== #\\n# BASIC CONFIGURATION\\n# ===================================================================== #\\nimages:\\n  # Try to use release-yyyyMMdd image if available. Note that release-yyyyMMdd will be removed after several months.\\n  - location: \\"https://cloud-images.ubuntu.com/releases/24.04/release-20240821/ubuntu-24.04-server-cloudimg-amd64.img\\"\\n    arch: \\"x86_64\\"\\n    digest: \\"sha256:0e25ca6ee9f08ec5d4f9910054b66ae7163c6152e81a3e67689d89bd6e4dfa69\\"\\n  - location: \\"https://cloud-images.ubuntu.com/releases/24.04/release-20240821/ubuntu-24.04-server-cloudimg-arm64.img\\"\\n    arch: \\"aarch64\\"\\n    digest: \\"sha256:5ecac6447be66a164626744a87a27fd4e6c6606dc683e0a233870af63df4276a\\"\\n  # Fallback to the latest release image.\\n  # Hint: run `limactl prune` to invalidate the cache\\n  - location: \\"https://cloud-images.ubuntu.com/releases/24.04/release/ubuntu-24.04-server-cloudimg-amd64.img\\"\\n    arch: \\"x86_64\\"\\n  - location: \\"https://cloud-images.ubuntu.com/releases/24.04/release/ubuntu-24.04-server-cloudimg-arm64.img\\"\\n    arch: \\"aarch64\\"\\n# CPUs\\n# \ud83d\udfe2 Builtin default: min(4, host CPU cores)\\ncpus: 4\\n# Memory size\\n# \ud83d\udfe2 Builtin default: min(\\"4GiB\\", half of host memory)\\nmemory: \\"12GiB\\"\\n# Disk size\\n# \ud83d\udfe2 Builtin default: \\"100GiB\\"\\ndisk: \\"120GiB\\"\\nmounts:\\n  - location: \\"~/\\"\\n    writable: true\\n# containerd is managed by Docker, not by Lima, so the values are set to false here.\\ncontainerd:\\n  system: false\\n  user: false\\nprovision:\\n  - mode: system\\n    # This script defines the host.docker.internal hostname when hostResolver is disabled.\\n    # It is also needed for lima 0.8.2 and earlier, which does not support hostResolver.hosts.\\n    # Names defined in /etc/hosts inside the VM are not resolved inside containers when\\n    # using the hostResolver; use hostResolver.hosts instead (requires lima 0.8.3 or later).\\n    script: |\\n      #!/bin/sh\\n      sed -i \'s/host.lima.internal.*/host.lima.internal host.docker.internal/\' /etc/hosts\\n  - mode: system\\n    script: |\\n      #!/bin/bash\\n      set -eux -o pipefail\\n      command -v docker >/dev/null 2>&1 && exit 0\\n      export DEBIAN_FRONTEND=noninteractive\\n      curl -fsSL https://get.docker.com | sh\\n      # NOTE: you may remove the lines below, if you prefer to use rootful docker, not rootless\\n      systemctl disable --now docker\\n      apt-get install -y uidmap dbus-user-session\\n  - mode: user\\n    script: |\\n      #!/bin/bash\\n      set -eux -o pipefail\\n      systemctl --user start dbus\\n      dockerd-rootless-setuptool.sh install\\n      docker context use rootless\\n  # Enable cpu emulation - https://docs.docker.com/build/building/multi-platform/#qemu\\n  - mode: system\\n    script: |\\n      #!/bin/bash\\n      set -eux -o pipefail\\n      docker run --privileged --rm tonistiigi/binfmt --install all\\nprobes:\\n  - script: |\\n      #!/bin/bash\\n      set -eux -o pipefail\\n      if ! timeout 30s bash -c \\"until command -v docker >/dev/null 2>&1; do sleep 3; done\\"; then\\n        echo >&2 \\"docker is not installed yet\\"\\n        exit 1\\n      fi\\n      if ! timeout 30s bash -c \\"until pgrep rootlesskit; do sleep 3; done\\"; then\\n        echo >&2 \\"rootlesskit (used by rootless docker) is not running\\"\\n        exit 1\\n      fi\\n    hint: See \\"/var/log/cloud-init-output.log\\". in the guest\\nhostResolver:\\n  # hostResolver.hosts requires lima 0.8.3 or later. Names defined here will also\\n  # resolve inside containers, and not just inside the VM itself.\\n  hosts:\\n    host.docker.internal: host.lima.internal\\ntimezone: Asia/Tokyo\\nportForwards:\\n  - guestSocket: \\"/run/user/{{.UID}}/docker.sock\\"\\n    hostSocket: \\"{{.Dir}}/sock/docker.sock\\"\\nmessage: |\\n  To run `docker` on the host (assumes docker-cli is installed), run the following commands:\\n  ------\\n  sudo /bin/ln -s -f {{.Dir}}/sock/docker.sock /var/run/docker.sock\\n  docker run hello-world\\n  ------\\n  It creates a symlink to the docker socket in the host\'s filesystem allowing seamless integration with docker commands.\\n  Optionally, run following command to enable building multi-arch images:\\n  ------\\n  docker buildx create --name container-builder --driver docker-container --bootstrap --use\\n  # then you can build multi-arch images like this:\\n  docker buildx build --platform linux/amd64,linux/arm64 -t <your-image-name> .\\n  ------\\nnetworks:\\n  # Lima can manage daemons for networks defined in $LIMA_HOME/_config/networks.yaml\\n  # automatically. The socket_vmnet binary must be installed into\\n  # secure locations only alterable by the \\"root\\" user.\\n  - lima: bridged\\n    # MAC address of the instance; lima will pick one based on the instance name,\\n    # so DHCP assigned ip addresses should remain constant over instance restarts.\\n    macAddress: \\"\\"\\n```\\n\\nA couple of noteworthy things about our template:\\n- We have a provision script to install Docker in rootless mode. This script comes from the default template provided by Lima for Docker.\\n- Port forwarding is set to mount the Docker socket inside the VM to the host.\\n- Steps shown in the message create a soft link. It links the default Docker socket file expected by the Docker clients to the socket file we mounted in the last step. This will help our clients transparently talk to the Docker daemon as if it were running natively on the host without further modifications.\\n- I\'m also using `bridged` networking. `socket_vmnet` should be installed for it to work. Check Lima docs for more info.\\n\\n\\nAlrighty, let\'s create the VM using above template. I\'m naming the VM `docker`. Assuming you create the template file at `/tmp/lima-docker.yaml`\\n```bash\\nlimactl create --name docker /tmp/lima-docker.yaml\\nlimactl start docker\\n```\\n\\nFollow the instructions printed by the start command to create a soft link for the Docker socket file and set up Docker buildx for multi-arch image building.\\n\\n## Next steps\\n1. After running `limactl create`, Lima should have created a `~/.lima/docker/lima.yaml` file. If you need to make any changes to your machine/template, update that file and restart the Lima machine.\\n2. You should be all set for using Docker. Start using Docker as if it\u2019s running natively.\\n```bash\\ndocker run --rm hello-world\\n\\nHello from Docker!\\nThis message shows that your installation appears to be working correctly.\\n\\nTo generate this message, Docker took the following steps:\\n 1. The Docker client contacted the Docker daemon.\\n 2. The Docker daemon pulled the \\"hello-world\\" image from the Docker Hub.\\n    (arm64v8)\\n 3. The Docker daemon created a new container from that image which runs the\\n    executable that produces the output you are currently reading.\\n 4. The Docker daemon streamed that output to the Docker client, which sent it\\n    to your terminal.\\n```\\n\\n3. (Optional) This setup can build and run images targeted for different platform. For example, M series Macbooks are ARM based (previous command outputted `(arm64v8)`). But you can run and build AMD images as well.\\n\\n```bash\\ndocker run --rm --platform linux/amd64 hello-world\\n\\nHello from Docker!\\nThis message shows that your installation appears to be working correctly.\\n\\nTo generate this message, Docker took the following steps:\\n 1. The Docker client contacted the Docker daemon.\\n 2. The Docker daemon pulled the \\"hello-world\\" image from the Docker Hub.\\n    (amd64)\\n 3. The Docker daemon created a new container from that image which runs the\\n    executable that produces the output you are currently reading.\\n 4. The Docker daemon streamed that output to the Docker client, which sent it\\n    to your terminal.\\n```\\n\\n\\n## References\\n- https://docs.docker.com/build/building/multi-platform/\\n- https://gist.github.com/fuzmish/df9eabf711c3f452ca19cce0621fc84e - gave the idea of using a symlink for the docker socket file\\n- https://lima-vm.io/docs/config/network/#vmnet-networks - for bridged networking"},{"id":"2024/the-internet-today","metadata":{"permalink":"/blog/2024/the-internet-today","source":"@site/../blogs/2024-10-09-the-internet-today.md","title":"2024-10-09 The Internet Today","description":"I started using the internet when I was in 10th grade, around 2010. My first exposure was through feature phones of that time, which could browse the web using some built-in browser. It was very expensive (around Rs.10/MB) and very slow (a couple of KBps). The reason for that anecdote is to say that I am by no means a veteran, but the pace at which things are moving in this space is incredible.","date":"2024-10-09T00:00:00.000Z","tags":[{"inline":true,"label":"2024-10","permalink":"/blog/tags/2024-10"},{"inline":true,"label":"2024","permalink":"/blog/tags/2024"}],"readingTime":5.11,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"2024-10-09 The Internet Today","slug":"2024/the-internet-today","tags":["2024-10","2024"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"2024-10-17 Docker On Macs","permalink":"/blog/2024/docker-on-macs"},"nextItem":{"title":"Space time complexities of recursive algorithms","permalink":"/blog/2024/recusive-space-time-complexities"}},"content":"I started using the internet when I was in 10th grade, around 2010. My first exposure was through feature phones of that time, which could browse the web using some built-in browser. It was very expensive (around Rs.10/MB) and very slow (a couple of KBps).\x3c!-- truncate --\x3e The reason for that anecdote is to say that I am by no means a veteran, but the pace at which things are moving in this space is incredible.\\n\\nThis technology is pretty awesome, but human greed and ego, in my opinion, are doing more harm than good (maybe that was a bit of an overstatement). It feels like the internet is not what it used to be and not what it could have been. There are too many walled gardens, unnecessary and frustrating hops, deceptive, manipulative, and sometimes outright illegal (or on the verge of that) activities being done, all in the name of \\"making the world a better place.\\"\\n\\nI\'ll try to keep this entry focused on the unnecessary and frustrating hops part. But first, let\'s go through some scenarios and see if we can spot the madness.\\n\\n## You want to send a letter to your friend who lives in another city. What do you do?\\n\\n1. You put it in a mail collection box.\\n2. The mailman collects it and takes it to the post office.\\n3. The letter is sorted and put on a vehicle.\\n4. The vehicle takes it to the destination city.\\n5. The letter is sorted again and delivered to your friend\'s mailbox.\\n6. Your friend takes it out and reads it.\\n\\n:::warning\\nFortunately, all these middlemen do not read your letters. But they can, without you or your friend even knowing. So always encrypt those letters if you are sending something sensitive.\\n:::\\n\\nThe internet or computer networks work in a similar fashion. You attach the destination address to your packet and send it to your router, which forwards it to its neighbor and so on until the destination is reached.\\n\\nBut there\'s a catch. The way things work today, you might be using, let\'s say WhatsApp, to send a message to your friend. Instead of sending it directly to your friend, it first goes to the WhatsApp server and then to your friend\'s phone. The WhatsApp server will store it indefinitely and can do whatever they want with that message, like reading it, deleting it, forwarding it to someone else, etc.\\n\\n:::warning\\nOn top of that, all of the other middlemen (routers) can also read all your packets. So encrypt everything; it doesn\'t cost much.\\n:::\\n\\nLet\'s take another example.\\n\\n## You want to send a letter to your neighbor. What do you do?\\nMaybe something like:\\n1. Put it in their mailbox.\\n2. Neighbor takes it out and reads it.\\n\\nOr maybe you are more social and decide to knock on their door, hand it over to them, and ask them how they are doing. But for argument\'s sake, let\'s say you are not and you just put it in their mailbox.\\n\\nNow, this seems pretty straightforward and efficient. But what happens when you do the same thing on the internet? All the steps that we saw in the previous example are repeated. Even though your neighbor is just a couple of meters away from you, your message might be traveling to a totally different continent to the central server, being stored there, and then being sent to your neighbor. And what happens if the internet cable connecting your country and the WhatsApp server is cut off (by the government or maybe by some sea creature)? Well, you guessed it, you can\'t send a message to your neighbor. Even though your town\'s, city\'s, and country\'s internet is working just fine.\\n\\nWant to go through another example?\\n\\n## You want to send a letter to your wife. What do you do?\\nMaybe you hand it over to her? Or maybe put it on the table? Or maybe put it in your home\'s mailbox?\\n\\nLet\'s say you like consistency and put it in the mailbox. Your wife then takes it out and reads it.\\n\\nBut what happens with the internet services we use today? Well, it\'s the same for all the cases. Your message has to travel thousands of kilometers just to come back to your own house.\\n\\n## Why am I rambling about all this?\\nIt is to show the absurdity of internet use today. I think most of the populace doesn\'t know how things work to even realize the absurdity of the situation and are at the mercy of the service providers who force them to go through all the hops all the time. They force them to go through all the risks that come with it. Risks of being monitored, manipulated, misinformed, regulated, censored, getting disconnected, etc.\\n\\nI remember the days when if I had to share a file with my friends, I could send it to them over Bluetooth. But today, the whole population is brainwashed into uploading the file somewhere first (maybe Google Drive, Dropbox, etc.), then sharing the link with people (over WhatsApp, FB, etc.), and then the recipient has to download it from there. People have moved from such open solutions to gated solutions like `AirDrop` (which works only with Apple devices), `Quick Share` (with Android only). Such stupidity.\\n\\n## What can we do?\\nWe used to have solutions, and they were very popular in the old days but are being pushed out by the big players. Just to name a few, we had:\\n1. Open protocols over Bluetooth for sharing and streaming.\\n2. Peer-to-peer networks and protocols - which are being actively attacked by the big players and governments today.\\n3. Amazing open protocols for sharing stuff: XMPP, SMTP, FTP, etc. - But nowadays, everyone is interested in pushing for their proprietary protocols and locking in users.\\n\\nI think we need to move towards a more open internet. With the adoption of IPv6, where every device can get a (or maybe thousands) unique address, we need to push for more direct connections and away from the centralized servers."},{"id":"2024/recusive-space-time-complexities","metadata":{"permalink":"/blog/2024/recusive-space-time-complexities","source":"@site/../blogs/2024-10-07-recursion-space-time-complexity.md","title":"Space time complexities of recursive algorithms","description":"Sometimes, I have difficulties visualizing the time complexities of recursive algorithms. For iterative ones, not considering the tricky notorious ones, most of the time I can just count the number of instructions and make some sense of it, but doing so for recursive algorithms overflows my brain\'s stack. I\'ll use this blog to jot down some ideas and approaches for doing complexity analysis on recursive algorithms.","date":"2024-10-07T00:00:00.000Z","tags":[{"inline":true,"label":"2024-10","permalink":"/blog/tags/2024-10"},{"inline":true,"label":"2024","permalink":"/blog/tags/2024"},{"inline":true,"label":"recursion","permalink":"/blog/tags/recursion"},{"inline":true,"label":"time complexity","permalink":"/blog/tags/time-complexity"},{"inline":true,"label":"daily","permalink":"/blog/tags/daily"}],"readingTime":8.4,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Space time complexities of recursive algorithms","slug":"2024/recusive-space-time-complexities","tags":["2024-10","2024","recursion","time complexity","daily"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"2024-10-09 The Internet Today","permalink":"/blog/2024/the-internet-today"},"nextItem":{"title":"Learnings From Calbridge","permalink":"/blog/2024/learnings-from-calbridge"}},"content":"Sometimes, I have difficulties visualizing the time complexities of recursive algorithms. For iterative ones, not considering the tricky notorious ones, most of the time I can just count the number of instructions and make some sense of it, but doing so for recursive algorithms overflows my brain\'s stack.\x3c!-- truncate --\x3e I\'ll use this blog to jot down some ideas and approaches for doing complexity analysis on recursive algorithms.\\n\\nI\'ll use some simple algorithms and deduce their analysis. This blog, by no means, is exhaustive and is mostly a primer and will, of course, not cover \\"clever\\" and intentionally notorious algorithms. So, let\'s get started with some algorithms.\\n\\n\\n## Factorial\\nA recursive implementation of factorial is\\n```python\\ndef factorial(n):\\n    if n < 2:\\n        return 1\\n    return n * factorial(n - 1)\\n```\\nThis basically calls itself `n` times before returning the result, and each call takes a constant time, so the complexity would be `O(n)`. Now, this was easy for my monkey brain, and I could do the pattern recognition and calculations in my head. But let\'s see if we can write it down systematically.\\n\\nLet\'s take some examples:\\n1. $t(1)$: We immediately return, complexity is $O(1)$ or let\'s denote it with $c$ for constant time.\\n2. $t(2)$: We do $2 \\\\times t(2 - 1)$ = Which is some constant time for multiplication + time for $t(2 - 1)$ = $c + t(1) = 2c$.\\n3. $t(3)$: We do $3 \\\\times t(3 - 1)$ = Some constant time for multiplication + time for $t(3 - 1)$ = $c + t(2) = c + 2c = 3c$.\\n\\nDo you see a pattern? No? Let\'s try again. Do you see the pattern? Still not? Okay, the pattern is:\\nFor $n$, time complexity would be = $nc$ or $O(n)$.\\n\\n## Fibonacci\\nLet\'s level up. Another simple recursive algorithm, but the complexity analysis of this one makes my head spin.\\n```python\\ndef fib(n):\\n    if n <= 2:\\n        return 1\\n    return fib(n - 1) + fib(n - 2)\\n```\\nAt first, it looks similar to the previous example, and my monkey brain tries to short-circuit it and think of it as $ \\\\sim 2 \\\\times \\\\text{fib}(n - 1) $, giving a complexity of $ O(n) $ - which is wrong.\\n\\nLet\'s see if we can deduce it as we did with the factorial.\\n1. $ t(1) $ or $ t(2) = c $\\n2. $ t(3) = t(2) + t(1) + c $ (some constant time for addition) = $ c + c + c = 3c $\\n3. $ t(4) = t(3) + t(2) + c = 3c + c + c = 5c $\\n4. $ t(5) = t(4) + t(3) + c = 5c + 3c + c = 9c $\\n\\nDo you see a pattern? Not yet? Let\'s do another one:\\n$ t(6) = t(5) + t(4) + c = 9c + 5c + c = 15c $\\n\\nDo you see it yet? Maybe not? And that\'s fine, as the pattern is not very explicit in this case because the problem is not equally divided, but the complexity is rising exponentially with some base between $ 1 $ and $ 2 $, and we can say it\'s $ O(2^n) $.\\n\\nIt\'s interesting to see how two seemingly similar algorithms have vastly different runtimes. BTW, if you were to write both of these algorithms iteratively, which is pretty straightforward as well, the time complexity for both would be $ O(n) $.\\n\\n## Notation\\nWe already used the $t(n)$ notation above, but we went from $t(1)$ to $t(n)$ and tried to find a pattern while doing so. We could have gone backwards as well, and it turns out that the pattern becomes more apparent while doing so, as we mostly keep talking in terms of $n$.\\n\\nLet\'s take the previous two examples for another spin.\\n### Factorial\\n$t(n) = c + t(n - 1) = c + (c + t(n - 2)) = 2c + t(n - 2) = 2c + (c + t(n - 3)) = 3c + t(n - 3)$\\n\\nDo you see the pattern? It is very evident in this case. The pattern is $k \\\\cdot c + t(n - k)$ where $k$ is $0 \\\\rightarrow n$. If we do $k = n$, then $t(n) = n \\\\cdot c + t(0) = n \\\\cdot c + c = O(n)$\\n\\n### Fibonacci\\n$t(n) = t(n - 1) + t(n - 2) + c$\\n\\n$t(n - 1)$ and $t(n - 2)$ would be similar, or at least when we talk about big-O notation, we can say $t(n - 1) \\\\approx t(n - 2)$ in this case, which gives us:\\n\\n$t(n) = 2 \\\\cdot t(n - 1) + c = 2 \\\\cdot (2 \\\\cdot t(n - 2) + c) + c = 2^2 \\\\cdot t(n - 2) + 2c + c$\\n\\n$= 2^2 \\\\cdot (2 \\\\cdot t(n - 3) + c) + 3c = 2^3 \\\\cdot t(n - 3) + 2^2c + 2c + c$\\n\\nHere, while the constant part is pretty significant, it\'s less than the first variable part and thus can be ignored when using big-O. So, let\'s rewrite it without the constant part.\\n\\n$t(n) = 2 \\\\cdot t(n - 1) = 2 \\\\cdot (2 \\\\cdot t(n - 2)) = 2^2 \\\\cdot t(n - 2) = 2^2 \\\\cdot (2 \\\\cdot t(n - 3)) = 2^3 \\\\cdot t(n - 3)$\\n\\nDo you see the pattern? Again, it\'s very explicit. The pattern is $2^k \\\\cdot t(n - k)$ where $k$ is $0 \\\\rightarrow n$. If we do $k = n$, then $t(n) = 2^n \\\\cdot t(0) = (2^n) \\\\cdot c = O(2^n)$\\n\\nThat was fun. Let\'s try this approach on a couple more algorithms.\\n\\n## Tower of Hanoi\\nThe Tower of Hanoi is a classic problem that involves three towers and you need to move the disks from the source to the target tower using the remaining tower. I won\'t go into the details of the problem and rules, but the recursive implementation would look something like this:\\n\\n```python\\n# Moving n disks from t1 to t2 using t3 as auxiliary\\ndef tower_of_hanoi(n, t1, t2, t3):\\n    if n == 1:\\n        print(f\\"Move disk 1 from {t1} to {t2}\\")\\n        return\\n    tower_of_hanoi(n - 1, t1, t3, t2)\\n    print(f\\"Move disk {n} from {t1} to {t2}\\")\\n    tower_of_hanoi(n - 1, t3, t2, t1)\\n```\\n\\nThe function `tower_of_hanoi` is called twice for each disk, except the last one, which gives us the recurrence relation:\\n\\n$ t(n) = 2 \\\\cdot t(n - 1) + c $\\n\\nNow, come to think of it, this looks simpler than the fibonacci example we saw earlier.\\n\\nEquation is exactly same as fibonacci. Let\'s not do the same drill again and conclude the complexity to be $ O(2^n) $.\\n\\n## Binary Tree Algorithms\\nThese things are looking to be easier than when I started. Let\'s try a couple more examples.\\n\\n### Tree Traversal\\n\\nTree traversal algorithms, such as in-order, pre-order, and post-order, visit each node in a binary tree exactly once. Here\'s in-order traversal:\\n\\n```python\\ndef in_order_traversal(node):\\n    if node is not None:\\n        in_order_traversal(node.left)\\n        print(node.value)\\n        in_order_traversal(node.right)\\n```\\n\\nI\'ve always argued that we need to visit each node exactly once, so the complexity is $O(n)$. But let\'s not cheat like that and stick to what we\'ve been doing so far. But, for simplicity, let\'s assume that the tree is balanced, i.e. each node has exactly two children. That will give us:\\n\\n$ t(n) = 2 \\\\cdot t(n / 2) + c $\\n\\nLet\'s do the same drill as we did for the previous examples.\\n\\n$ t(n) = 2 \\\\cdot t(n / 2) + c = 2 \\\\cdot (2 \\\\cdot t(n / 4) + c) + c = 2^2 \\\\cdot t(n / 4) + 2c + c = 2^2 \\\\cdot (2 \\\\cdot t(n / 8) + c) + 3c = 2^3 \\\\cdot t(n / 8) + 2^2c + 2c + c $\\n\\nDo you see the pattern? Actually let\'s get rid of the constants and simplify it.\\n\\n$ t(n) = 2 \\\\cdot t(n / 2) = 2 \\\\cdot (2 \\\\cdot t(n / 4)) = 2^2 \\\\cdot t(n / (2^2)) = 2^2 \\\\cdot (2 \\\\cdot t(n / 8)) = 2^3 \\\\cdot t(n / (2^3)) $\\n\\n$ t(n) = 2^k \\\\cdot t(n / (2^k)) $\\n\\nIf we set $ k = \\\\log n $, we can get rid of $ t() $ on right side and get\\n\\n$ t(n) = 2^{\\\\log n} \\\\cdot t(1) = n \\\\cdot c = O(n) $\\n\\nWow, that was fun. Let\'s do one more and then call it a day.\\n\\n### Binary Tree Search\\n\\nSearching for a value in a balanced binary tree\\n\\n```python\\ndef search_tree(node, value):\\n    if node is None or node.value == value:\\n        return node\\n    if value < node.value:\\n        return search_tree(node.left, value)\\n    else:\\n        return search_tree(node.right, value)\\n```\\n\\nThis one is interesting. It looks like the previous one, but it\'s not; because we only go either left or right. The recurrence relation is:\\n\\n$ t(n) = t(n / 2) + c $\\n\\n$ t(n) = t(n / 2) + c = (t(n / 4) + c) + c = (t(n / 8) + c) + c + c = t(n / (2^3)) + 3*c $\\n\\nPattern is $ t(n / (2^k)) + k \\\\cdot c $.\\n\\nIf we set $ k = \\\\log n $, we can get rid of $ t() $ on right side and get\\n\\n$ t(n) = t(n / (2^{\\\\log n})) + \\\\log n \\\\cdot c = t(1) + \\\\log n \\\\cdot c = c + \\\\log n \\\\cdot c = O(\\\\log n) $\\n\\n\\n## Next\\n\\nHopefully, this will help me kickstart the complexity analysis of some simple algorithms that follow straightforward patterns or have a fixed number of recursive calls at each recursion. Of course, it might not work or would be hard to generalize this for some \\"complex\\" algorithms, like the graph algorithms. There\'s also [Master\'s Theorem](https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)), which helps with these complexity analyses, but for the life of me, I can\'t seem to remember it and don\'t find it intuitive enough. But check that out if it works for you."},{"id":"2024/learnings-from-calbridge","metadata":{"permalink":"/blog/2024/learnings-from-calbridge","source":"@site/../blogs/2024-08-27-calbridge-learnings.md","title":"Learnings From Calbridge","description":"Development of calbridge is going on with full momentum now. I\'ve been spending hours working on it during nights. Just kidding. I mean I\'ve spent couple of nights working on it but then took some time off. And then wanted to write this blog before picking it up again. For the uninitiated, calbridge is a utility I\'ve been working on to integrate (or bridge) my Caldav server with my mail server. More background here, if you are interested.","date":"2024-08-27T00:00:00.000Z","tags":[{"inline":true,"label":"2024-08","permalink":"/blog/tags/2024-08"},{"inline":true,"label":"2024","permalink":"/blog/tags/2024"},{"inline":true,"label":"calbridge","permalink":"/blog/tags/calbridge"},{"inline":true,"label":"learning","permalink":"/blog/tags/learning"},{"inline":true,"label":"caldav","permalink":"/blog/tags/caldav"},{"inline":true,"label":"smtp","permalink":"/blog/tags/smtp"},{"inline":true,"label":"imap","permalink":"/blog/tags/imap"}],"readingTime":10.055,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Learnings From Calbridge","slug":"2024/learnings-from-calbridge","tags":["2024-08","2024","calbridge","learning","caldav","smtp","imap"]},"unlisted":false,"prevItem":{"title":"Space time complexities of recursive algorithms","permalink":"/blog/2024/recusive-space-time-complexities"},"nextItem":{"title":"Basic Auth and Digest","permalink":"/blog/2024/basic-auth-and-digest"}},"content":"Development of [calbridge](https://github.com/nakamorg/calbridge) is going on with full momentum now. I\'ve been spending hours working on it during nights.\x3c!-- truncate --\x3e Just kidding. I mean I\'ve spent couple of nights working on it but then took some time off. And then wanted to write this blog before picking it up again. For the uninitiated, **calbridge** is a utility I\'ve been working on to integrate (or bridge) my Caldav server with my mail server. More background [here](/journal/2024/caldav-and-mail), if you are interested.\\n\\nAnyways, I wanted to take some time and jot down my thoughts and learnings from this project so far.\\n\\n## Using LLMs\\nI used `Claude Opus` to bootstrap the project. It provided me with the initial structure of the project. And then for every core functionality that I needed, it provided me with the starter code. I found it to be very helpful. It got me over the massive inertia of starting something from scratch and gave a boilerplate to start with. Although, the majority of the code it provided had to be re-written, it still helped a lot - as it gave pretty good hints on what libs to use and how to approach the problem and acted as a dummy duck or someone to yell on during debugging (trust me I swore a lot and it took it like a champ).\\n\\n## Auth\\nFirst thing that I came across and learned about was the `Digest Auth` and have already written a blog about that [here](/blog/2024/basic-auth-and-digest). I started with making simple `http` calls to the server to fetch my calendars and it seemed to have worked once the auth thing got fixed. But I soon found myself wanting more (like correctly parsing and handling calendar data). I was a bit hesitant to include too many dependencies to the project at first but ultimately gave up on that design goal as I wanted to have something working as soon as possible instead of spending too much time getting it perfect. Which brings us to our next topic. Packages.\\n\\n:::note\\nIt\'s okay to start with an \\"un-optimized\\" solution and have it working instead of spending too much energy building the perfect product in the first try. Once you have something working, you can iterate on it if you want.\\n:::\\n\\n## Packages\\nI looked for packages a lot. All I wanted were thin client libs for caldav, smtp and imap. But for caldav, at least, most of the libs were either abondoned or were providing both the server and the client. Ultimately, I settled with https://github.com/emersion - it came with webdav, caldav client and server (talking about thin clients). Thank you **emersion** for providing such a high quality implementation of these protocols. **emersion** provided all the libs I needed. While the libs were lacking in documentation and some working examples, they were implementing the target protocols (caldav, imap etc) to the point and using same terminology for variable, function names etc. as in the protocl spec. So, it didn\'t take much effort (if you don\'t consider 2-3 hours of fumbling around **much**) to write something working.\\n\\n:::note\\nAnd much of this time could have been saved if I had looked at github issues earlier.\\n:::\\n\\nNext, I\'ll discuss about some of these issues in detail.\\n\\n## golang http.request - Cannot reuse\\nThe caldav lib I used provided a neat way of passing in a custom HTTPClient. This way I could implemet a custom `digest auth` client to authenticate with my server. A working implementation for **digest auth** need to make at least two calls to the server. First a dummy call, just to get the authorization challenge from the server and then the actual call with the authorization header set. I reused the same `http.Request` in both calls.\\n\\nThe second call silently failed. The returned `http.Response` was `nil` without any errors. I thought there was something wrong with the client, my digest auth implementation or the caldav server. I inserted a bunch of `Print` statements to debug it (now you know what I use for debugging). And then added some more **print** statements and realized that `Response` is always `nil`. I had no idea what might have caused it. Out of desperation, I passed the function context to my GPT and asked it why the response is nil all the time. It suggested not to re-use the previous `response` object. I told it not to be stupid. It apologized and said the the same request shouldn\'t be re-used. That made some sense to me and I made a clone of the previous request with `req.Clone(context.Background())` but it still didn\'t work though there were some error response from the server this time. Some progress, eh? Server complained that I were not asking for any data. I printed the original request and could see that I was indeed asking for some data. Then I printed the cloned request and noticed that it didn\'t have the `body`. Hmm, what kind of `clone` was it?\\n\\nTurns out that the `req.Clone` doesn\'t clone the request body. Here\'s the comment for that method\\n> Clone returns a deep copy of r with its context changed to ctx. The provided ctx must be non-nil.\\n>\\n> For an outgoing client request, the context controls the entire lifetime of a request and its response: obtaining a connection, sending the request, and reading the response headers and body.\\n\\nNowhere does it mentions that it won\'t clone the body. It even says that it makes a deep copy. But the entire game is of the `context`. Once the context is done, it seems the Body is done for or consumed. So, I needed to make a copy of the body before sending out the request and used that copy to set the body of the cloned request. `req.GetBody()` helped in making the copy of the request.\\n\\nAnd that was it. The server started returning my calendars after that.\\n\\n:::note\\nThat\'s not entirely true. During debugging, I thought that maybe I\'m not sending the correct queries to the server. So I tried with bunch of different queries and settings. And then spent some extra minutes to realize that I\'ve messed up the queries and then some more to correcting those.\\n:::\\n\\n## SMPT - Just use an external package\\nI started with `net/smtp` package that is included with the standard Golang installation. There was nothing wrong with it but it seems that its development is frozen and the authors suggested using some other maintained lib. So, after writing a working implementation using this \\"obselete\\" lib, I searched for another lib and then re-wrote the thing to make it compatible with the new lib.\\n\\nOne thing I solidified during this experience was to provide a good interface to your users. This allows to change your implementation as you want (change libs/packages or re-write whole thing by youself) as long as you don\'t alter the user interface. I started with `NewSMTPClient` function to return a custom (hidden) client object and a `SendCalendarInvite(calObject caldav.CalendarObject)` method for the client. The client didn\'t expose its fields directly to the end user - so it was pretty easy to switch to another package for my `SendCalendarInvite` implementation without making any changes to the front-end.\\n\\n## IMAP - Sweet time that we spent together\\nI spent about 90 minutes on a very \\"stupid\\" mistake. Here\'s how. So I needed to ask my mail server for all the emails from last few hours. Then I could process them to see if they had any calendar invite. `github.com/emersion/go-imap` lib along with `github.com/emersion/go-message/mail` make this whole thing a no-brainer given that you have some brain cells to correctly use these packages or you are willing to spend some time looking at protocol RFCs or maybe just search Github issues for the problems you\'r facing.\\n\\nHere\'s what happened. I managed to find the sequence numbers of the emails that I needed. But didn\'t check if the returned result was empty or not. Sending the empty sequence numbers to server to fetch those results returned weird errors from the server which gave no hint about the actual issue.\\n```go\\ncriteria.SentSince = time.Now().Add(-4*hours)\\nseqNums, err := c.Search(criteria)\\n\\nitems := []imap.FetchItem{imap.FetchBody}\\nmsgs := make(chan *imap.Message, len(seqNums))\\nseqSet := new(imap.SeqSet)\\nseqSet.AddNum(seqNums...)\\nif err := c.Fetch(seqSet, items, msgs); err != nil {\\n    return nil, fmt.Errorf(\\"failed to fetch email: %v\\", err)\\n}\\n```\\n\\nSo, the first thing that needed to be done was not to query the server if you didn\'t have anything to query for.\\n```go\\nseqNums, err := c.Search(criteria)\\n\\nif len(seqNums) == 0 {\\n    return []string{}, nil\\n}\\n```\\n\\nAlright, that makes sense now. So I increased my search criteria to 10 hours and was pretty certain that I had several mails in that window. And that resulted in segmentation fault. Meaning I was trying to read some memory location which I had no business reading. Fortunately, unlike `c`, golang gives you some hints about the crime location. It pointed out to me the line number where I was trying to read the mail body.\\n\\nLet\'s look at the code again (with just the necessary parts)\\n```go\\nitems := []imap.FetchItem{imap.FetchBody}\\nmsgs := make(chan *imap.Message, len(seqNums))\\nseqSet := new(imap.SeqSet)\\nseqSet.AddNum(seqNums...)\\nif err := c.Fetch(seqSet, items, msgs); err != nil {\\n    return nil, fmt.Errorf(\\"failed to fetch email: %v\\", err)\\n}\\n```\\n`items` that I want to fetch is the email body (`imap.FetchBody` here is the string `BODY`) - as that\'s the thing that would have the calendar invites I\'m after. But the body field of all the emails that it fetched was always `nil`. It was weird. I had explicitly specified to fetch the `BODY` and the body was nil. Then my monkey brain told me not to optimize and just fetch everything. Conveniently, there was `imap.FetchAll` and I thought it would fetch everything. But NO. Both of these options fetched everthing but the email body. After plucking another 100 hair from my head and I surrendered and looked online. Within a minute or so, I came across this github comment `https://github.com/emersion/go-imap/issues/306#issuecomment-546532174` - turned out I had to fetch `BODY.PEEK[]`. Fetch `ALL` or `BODY` do not fetch everthing or the body respectively (as one would have expected) but the headers and stuff. So, folks please read those RFCs.\\n\\n:::note\\nPeek keeps the email unread, so one can use `BODY[]` as well if they want to mark the mail read. (Look at the smart me)\\n:::\\n\\nSo, here\'s the wokring code\\n```go\\nitems := []imap.FetchItem{imap.FetchItem(\\"BODY.PEEK[]\\")}\\n.. removed for brevity ..\\nif err := c.Fetch(seqSet, items, msgs); err != nil {\\n    return nil, fmt.Errorf(\\"failed to fetch email: %v\\", err)\\n}\\n```\\n\\nFunnily enough, when I started working on the IMAP functionlity. I asked `Claude Opus` (a gpt) to write me the code, it suggested the folowing\\n```go\\nsection := &imap.BodySectionName{}\\nitems := []imap.FetchItem{section.FetchItem()}\\n```\\n\\nwhich translates to a working approach\\n```go\\nitems := []imap.FetchItem{imap.FetchItem(\\"BODY[]\\")}\\n```\\nBut I became a smart-ass and asked why it was making it so complicated. We needed just the body and `FetchBody` provided that, so why couldn\'t we use that instead? The GPT, like it always does, apologized to me, praised me for being so smart, and suggested that we could indeed just use `FetchBody`. Sometimes I think it did that on purpose, maybe to hold some grudge or something /s.\\n\\n## Something to end the blog with\\nLike I mentioned in the begining, I wanted to keep the final binary very small and might end up re-writing some of the functionalities that the packages provide by myself. As of now, I think that the webdav dependency (it provides caldav) can be easily do away with. As it just needs to make some simple http calls to the server. So that\'s something I\'ve been looking towards getting rid of. But not until I have a fully working solution ready first.\\n\\n:::note\\nI loved using these **note** sections in this blog entry. Sometimes at places where it didn\'t make any sense. And now this blog entry being done, maybe I can go back actually implementaing the real thing.\\n:::"},{"id":"2024/basic-auth-and-digest","metadata":{"permalink":"/blog/2024/basic-auth-and-digest","source":"@site/../blogs/2024-08-22-digest-auth.md","title":"Basic Auth and Digest","description":"I\'ve started working on calbridge and spent some time working on the caldav client part of it. While connecting to the caldav server, I passed in the creds as basic auth. The server repeatedly returned 401 - Unauthorized. I might have spent roughly an hour figuring out where I might be sending the wrong username, password or the server url.","date":"2024-08-22T00:00:00.000Z","tags":[{"inline":true,"label":"2024-08","permalink":"/blog/tags/2024-08"},{"inline":true,"label":"2024","permalink":"/blog/tags/2024"},{"inline":true,"label":"digest","permalink":"/blog/tags/digest"},{"inline":true,"label":"basic auth","permalink":"/blog/tags/basic-auth"}],"readingTime":3.675,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Basic Auth and Digest","slug":"2024/basic-auth-and-digest","tags":["2024-08","2024","digest","basic auth"]},"unlisted":false,"prevItem":{"title":"Learnings From Calbridge","permalink":"/blog/2024/learnings-from-calbridge"},"nextItem":{"title":"Terraform Provider Dynamic Configuration","permalink":"/blog/2024/terraform-provider-dynamic-configuration"}},"content":"I\'ve started working on [calbridge](/journal/2024/caldav-and-mail) and spent some time working on the caldav client part of it.\x3c!-- truncate --\x3e While connecting to the caldav server, I passed in the creds as basic auth. The server repeatedly returned `401 - Unauthorized`. I might have spent roughly an hour figuring out where I might be sending the wrong username, password or the server url.\\n\\nAnd then by luck or whatever, I stumbled across this part of the response\\n```xml\\n<?xml version=\\"1.0\\" encoding=\\"utf-8\\"?>\\n<d:error xmlns:d=\\"DAV:\\" xmlns:s=\\"http://sabredav.org/ns\\">\\n  <s:sabredav-version>4.6.0</s:sabredav-version>\\n  <s:exception>Sabre\\\\DAV\\\\Exception\\\\NotAuthenticated</s:exception>\\n  <s:message>No \'Authorization: Digest\' header found. Either the client didn\'t send one, or the server is misconfigured</s:message>\\n</d:error>\\n```\\n**No \'Authorization: Digest\' header found** - the poor fella was expecting the `Authorization: Digest` and I was passing in the `Authorization: Basic`.\\n\\n:::tip\\nIf you are using curl, pass the `--digest` flag and it will handle the **digest** auth flow for you. \\n:::\\n\\nI\'ve heard of this auth mechanism before but never went into much details of what it is, why it is and how it is. These three questions I\'ll try to cover in this blog. Maybe not in much details but something to keep it interesting and informative\\n\\n## What\\nSo, it\'s a HTTP auth mechanism like basic auth. But unlike basic auth, where username and password are transmitted in plain-text and can be intercepted and decoded by anyone on the network (if you are using HTTP (not HTTPS)); with auth digest you never send the username and password over. Instead you use your creds to solve a puzzle that the server sents you. Now, of course, if you are not on HTTPS - an attacker on the network can easily intercept this solution to the puzzle and then impersonate you. But at least they won\'t have your password.\\n\\nMoral of the story: Always use a secured connection (HTTPS) if you are sending over sensitive information.\\n\\n## Why\\nI think the last section kinda answered this question. We needed it to avoid sending our passwords in plain-text over an HTTP connection.\\n:::info\\nA question for you: What do you think is the best way to create a user, in the first place, over such a connection?\\n:::\\n\\n## How\\nI\'ll try to gloss over most of the technical details but still keep enough to satiate those of you who dig these kind of details. I\'ll use some Golang code to show the implementation details\\n1. Client asks the server for a protected resource (without sending any creds)\\n    ```go\\n    req, _ := http.NewRequest(\\"GET\\", URL, nil)\\n    resp, _ := http.DefaultClient.Do(req)\\n    ```\\n2. Server responds with `StatusUnauthorized (401)` status code and sends the challenge in `WWW-Authenticate` header\\n    ```go\\n    if resp.StatusCode == http.StatusUnauthorized {\\n        challenge := resp.Header.Get(\\"WWW-Authenticate\\")\\n\\n        req, _ = http.NewRequest(\\"GET\\", URL, nil)\\n\\n        // Set the Digest authentication header\\n        req.Header.Set(\\"Authorization\\", getDigestAuthorization(challenge, Username, Password, \\"GET\\", URL))\\n\\n        // Send the request with Digest authentication\\n        resp, _ = http.DefaultClient.Do(req)\\n    }\\n    ```\\n3. In `WWW-Authenticate`, server sends couple of fields like `realm`, `nonce` and these are used together with username and password to generate a hash. This hash is what is sent back to the server in `Authorization` header\\n    ```go\\n    func getDigestAuthorization(challenge, username, password, method, uri string) string {\\n        // Parse the Digest challenge\\n        fields := parseDigestChallenge(challenge)\\n\\n        realm := fields[\\"realm\\"]\\n        nonce := fields[\\"nonce\\"]\\n\\n        // Generate the response hash\\n        ha1 := getMD5(fmt.Sprintf(\\"%s:%s:%s\\", username, realm, password))\\n        ha2 := getMD5(fmt.Sprintf(\\"%s:%s\\", method, uri))\\n        response := getMD5(fmt.Sprintf(\\"%s:%s:%s\\", ha1, nonce, ha2))\\n\\n        // Construct the Digest authorization header\\n        authParams := fmt.Sprintf(\\n            `username=\\"%s\\", realm=\\"%s\\", nonce=\\"%s\\", uri=\\"%s\\", response=\\"%s\\"`,\\n            username, realm, nonce, uri, response\\n        )\\n        return \\"Digest \\" + authParams\\n    }\\n    ```\\n4. As you might have noticed, we send back the username, realm, nonce and the hash to the server but not the password. Using the username, the server looks up the password from its db and does this calculation on its side. If the hash matches to the server calculated hash then the request is authenticated otherwise not.\\n\\nThat was it about the digest auth. A clever way if proving you are who you say you are without sending over your password. Again it might not be necessary if you use HTTPS and might not be very secure(citations needed) but it\'s still much better than basic auth.\\n\\n:::warning\\nI\'ve removed lots of implementation details from the code snippets above. Do not copy paste this code for your implementation of digest auth. Use some existing library or follow the official RFC or spec of the protocol.\\n:::"},{"id":"2024/terraform-provider-dynamic-configuration","metadata":{"permalink":"/blog/2024/terraform-provider-dynamic-configuration","source":"@site/../blogs/2024-08-20-terraform-providers.md","title":"Terraform Provider Dynamic Configuration","description":"I had a requirement where I wanted to work with Honeycombio\'s terraform provider. Unlike Datadog, Honeycomb has the concept of environments. It maps perfectly with our infra environments, i.e we can send prd telemetry to prd honeycomb environment and dev to dev.","date":"2024-08-20T00:00:00.000Z","tags":[{"inline":true,"label":"2024-08","permalink":"/blog/tags/2024-08"},{"inline":true,"label":"2024","permalink":"/blog/tags/2024"},{"inline":true,"label":"terraform","permalink":"/blog/tags/terraform"},{"inline":true,"label":"provider","permalink":"/blog/tags/provider"},{"inline":true,"label":"honeycomb","permalink":"/blog/tags/honeycomb"}],"readingTime":2.085,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Terraform Provider Dynamic Configuration","slug":"2024/terraform-provider-dynamic-configuration","tags":["2024-08","2024","terraform","provider","honeycomb"]},"unlisted":false,"prevItem":{"title":"Basic Auth and Digest","permalink":"/blog/2024/basic-auth-and-digest"},"nextItem":{"title":"Github Actions for the Blog","permalink":"/blog/2024/github-actions"}},"content":"I had a requirement where I wanted to work with Honeycombio\'s terraform provider. Unlike Datadog, Honeycomb has the concept of environments. It maps perfectly with our infra\x3c!-- truncate --\x3e environments, i.e we can send prd telemetry to prd honeycomb environment and dev to dev.\\n\\nHoneycomb provider uses API keys for configuration and each environment has its own keys. That meant that whenever we wanted to run our terraform code, we needed to set some env vars to specify the correct environment\'s api key. This gets particularly messy on our CI server, as we have a single instance to handle all our environments. When we ran a CI pipeline, there definetely is a way to specify which env it should target and we can very easily use that information in our terraform code. But terraform doesn\'t provide a native way to read any env vars. It reads `TF_VARS_xx` and target providers can read whetever env vars they expect to configure themselves. In case of Honeycomb, it\'s `HONEYCOMB_API_KEY`. But we can\'t mutate this nev var on CI server every time we run a job/pipeline, as there might be multiple jobs, simultaneoulys taregting both the dev and prd environments.\\n\\n## Solution\\nWe decided to store the API keys in AWS Secrets Manager. Our dev aws account stores the key for dev honeycomb env and prd for prd. We already have tooling in place to pass in the env specific AWS role when running the pipeline, so the terraform aws provider would be configured properly. \\n\\n:::note\\nWe could, of couse, modify the tooling to support this Honeycomb case in a similar way. But that didn\'t sound like a good idea. As it sets the precedent of doing this for all the providers we use or will use.\\n:::\\n\\nSo, the solution is simple. We use aws provider to fetch the target account\'s API key and use those to configure the honey-comb provider. Here\'s the code snippet\\n\\n```json title=\\"provider.tf\\"\\nterraform {\\n  required_providers {\\n    honeycombio = {\\n      source  = \\"honeycombio/honeycombio\\"\\n      version = \\"~> 0.26.0\\"\\n    }\\n    aws = {\\n        source = \\"hashicorp/aws\\"\\n    }\\n  }\\n}\\n\\ndata \\"aws_secretsmanager_secret\\" \\"honeycomb\\" {\\n  name = \\"<name of aws secret holding honeycomb api key>\\"\\n}\\n\\ndata \\"aws_secretsmanager_secret_version\\" \\"honeycomb\\" {\\n  secret_id = data.aws_secretsmanager_secret.honeycomb.id\\n}\\n\\nlocals {\\n  honeycomb_configuration_key = try(jsondecode(data.aws_secretsmanager_secret_version.honeycomb.secret_string)[\\"configuration_key\\"], null)\\n}\\n\\nprovider \\"honeycombio\\" {\\n  api_key = local.honeycomb_configuration_key\\n}\\n```\\n\\nThe same idea can be extended to other similar providers or to specify different configuration depending on the environemt, e.g: dev key with limited access and prd key with full access."},{"id":"2024/github-actions","metadata":{"permalink":"/blog/2024/github-actions","source":"@site/../blogs/2024-08-16-github-actions.md","title":"Github Actions for the Blog","description":"Thinking about adding github actions to build and deploy this blog instead of running them on a VM on my machine. For some reason I couldn\'t get myself to install nodejs and npm on my regular workstation. Maybe I should do that as well.","date":"2024-08-16T00:00:00.000Z","tags":[{"inline":true,"label":"2024-08","permalink":"/blog/tags/2024-08"},{"inline":true,"label":"2024","permalink":"/blog/tags/2024"},{"inline":true,"label":"github action","permalink":"/blog/tags/github-action"}],"readingTime":1.89,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Github Actions for the Blog","slug":"2024/github-actions","tags":["2024-08","2024","github action"],"hide_table_of_contents":true},"unlisted":false,"prevItem":{"title":"Terraform Provider Dynamic Configuration","permalink":"/blog/2024/terraform-provider-dynamic-configuration"},"nextItem":{"title":"Reading List","permalink":"/blog/2024/reading-list"}},"content":"Thinking about adding github actions to build and deploy this blog instead of running them on a VM on my machine. For some reason\x3c!-- truncate --\x3e I couldn\'t get myself to install nodejs and npm on my regular workstation. Maybe I should do that as well.\\n\\nBut for the time being I\'m going to try my luck with github actions. Just added a github workflow file and pushing this blog file to master should trigger that. So, let get triggering!\\n\\nSo, that worked. But only halfway. The last step `npm run deploy` failed asking for my github username or specifying using ssh key. I was hoping that it would know how to push the changes as it was already running in github action. Let\'s see if `Claude Opus` can help us figure this out. After multiple back and forth it suggested to use this snippet\\n```yaml\\n- name: Deploy\\n    run: |\\n    npm run deploy\\n    working-directory: docusaurus\\n    env:\\n    GIT_USER: github-actions\\n    GIT_PASS: ${{ secrets.GITHUB_TOKEN }}\\n```\\nI think it should work. I\'m just not confident about the `GIT_USER` part. Should I use my github username there instead? Anyways, let\'s try pushing and see how it goes.\\n\\nThe action run was succesful but it still didn\'t deploy, complaining about setting git user email and stuff. So, let\'s try with\\n\\n```yaml\\n- name: Deploy\\n  run: |\\n    git config --global user.email \\"${{ github.actor }}@users.noreply.github.com\\"\\n    git config --global user.name \\"${{ github.actor }}\\"\\n    npm run deploy\\n  working-directory: docusaurus\\n  env:\\n    GIT_USER: ${{ github.actor }}\\n    GIT_PASS: ${{ secrets.GITHUB_TOKEN }}\\n```\\n\\nThat was it. The blog is \\"on\\" now. All I have to do is write and push to master and github will take care of build, deploy and hosting. Good times. You can find the complete and up-to-date code in github workflows of [this repo](https://github.com/nakamorg/nakamorg.github.io). One more thing though, I\'m not sure if the caching for node modules is working. Initial runs of `npm ci` took about 10 secs and the latest one with a cache hit took 6 secs - so maybe it\'s working? Maybe I can try doing `ls node_modules` in the github action and see or maybe there are other better options? But let\'s do that some other time. It\'s almost 19:18 and I need to get off working and start my work out."},{"id":"2024/reading-list","metadata":{"permalink":"/blog/2024/reading-list","source":"@site/../blogs/2024-08-15-reading-list.md","title":"Reading List","description":"I\'ve been thinking about managing a reading list, basically a list of urls or excerpts as I read them or get to know about them. Previously, I set up Linkwarden for this but didn\'t use it much (or at all). Hoping that this rlog (running blog) can substitute that. Hopefully, I\'ll keep it updated with everything insteresting I read on day to day basis, but it\'ll mostly be media that I\'d consume on my workstation (same machine I use to write this blog). Anyways, something is better than nothing. off we go","date":"2024-08-15T00:00:00.000Z","tags":[{"inline":true,"label":"reading","permalink":"/blog/tags/reading"},{"inline":true,"label":"reading-list","permalink":"/blog/tags/reading-list"}],"readingTime":1.3,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Reading List","slug":"2024/reading-list","tags":["reading","reading-list"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Github Actions for the Blog","permalink":"/blog/2024/github-actions"},"nextItem":{"title":"Started a second blog","permalink":"/blog/2024/started-second-blog"}},"content":"I\'ve been thinking about managing a reading list, basically a list of urls or excerpts as I read them or get to know about them. Previously, I set up Linkwarden\x3c!-- truncate --\x3e for this but didn\'t use it much (or at all). Hoping that this rlog (running blog) can substitute that. Hopefully, I\'ll keep it updated with everything insteresting I read on day to day basis, but it\'ll mostly be media that I\'d consume on my workstation (same machine I use to write this blog). Anyways, something is better than nothing. off we go\\n\\n## 2024-08-15\\n15th August - Independence Day, India\\n- [medium.com/yandex/good-retry-bad-retry-an-incident-story](https://medium.com/yandex/good-retry-bad-retry-an-incident-story-648072d3cee6) - Very interesting and fun read. Goes into the details of when and why `retries` should be used. How simple or simple exponetial backoff retries could be major issue when is system is recovering from downtime. Sugeests to use retries with `Retry Budget`, `Circuit Breaker`, `Load Shedding`, `Deadline Propogation` etc. Former two are implemented on client whereas later two on servers.\\n\\n## 2024-08-16\\n- [isovalent.com/blog/post/demystifying-cni](https://isovalent.com/blog/post/demystifying-cni) - A short article briefly discusses the container runtime and network interface and goes on to build a CNI plugin using bash. Apparently, you only need two files. A conf file at `/etc/cni/net.d/` and a CNI executable at `/opt/cni/bin/`. CRI loads the first conf file and finds the corresponding executable and then calls the CNI executable as per the CNI spec.\\n\\n## 2024-08-29\\n- [CORS is stupid](https://kevincox.ca/2024/08/24/cors/) - For the last eight years of my career, this is the only post that made me understand cors and how it affects users."},{"id":"2024/started-second-blog","metadata":{"permalink":"/blog/2024/started-second-blog","source":"@site/../blogs/2024-08-08-second-blog.md","title":"Started a second blog","description":"For some time, I wanted to start with a daily blog. Yes, I know I started this blog about four months ago and have only two entries as of now (including this one). Guess I\'m mostly into wasting my time finding which blog genertors to use and starting up a blog site but not into writing. Anyways, I wanted a new blog where I could write freely, without going into much details of the things, about my daily life and routine. I\'ve set up a docusaurus blog previously (the one you are currently reading). I spent some time checking if the mdbook would be a good fit for this daily journal blog. Fired up a test mdbook book and compared it with docusaurus style blog I had. Docusaurus style felt good to eyes (it looked nice) - so I decided to stick with docusaurus and started a second blog on same docusaurus instance. You can find it at /journal address of this site.","date":"2024-08-08T00:00:00.000Z","tags":[{"inline":true,"label":"2024-08","permalink":"/blog/tags/2024-08"},{"inline":true,"label":"2024","permalink":"/blog/tags/2024"},{"inline":true,"label":"journal","permalink":"/blog/tags/journal"},{"inline":true,"label":"daily","permalink":"/blog/tags/daily"},{"inline":true,"label":"multiple blogs","permalink":"/blog/tags/multiple-blogs"},{"inline":true,"label":"rss","permalink":"/blog/tags/rss"},{"inline":true,"label":"custom css","permalink":"/blog/tags/custom-css"}],"readingTime":4.855,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Started a second blog","slug":"2024/started-second-blog","tags":["2024-08","2024","journal","daily","multiple blogs","rss","custom css"]},"unlisted":false,"prevItem":{"title":"Reading List","permalink":"/blog/2024/reading-list"},"nextItem":{"title":"Welcome","permalink":"/blog/2024/blogging-platform"}},"content":"For some time, I wanted to start with a daily blog. Yes, I know I started this blog about four months ago and have only two entries as of now (including this one). Guess I\'m mostly into wasting\x3c!-- truncate --\x3e my time finding which blog genertors to use and starting up a blog site but not into writing. Anyways, I wanted a new blog where I could write freely, without going into much details of the things, about my daily life and routine. I\'ve set up a docusaurus blog previously (the one you are currently reading). I spent some time checking if the **mdbook** would be a good fit for this daily journal blog. Fired up a test mdbook book and compared it with docusaurus style blog I had. Docusaurus style felt good to eyes (it looked nice) - so I decided to stick with docusaurus and started a second blog on same docusaurus instance. You can find it at [/journal](/journal) address of this site.\\n\\n## Detail about the setup\\nThis blog lives at [nakamorg.github.io](https://github.com/nakamorg/nakamorg.github.io) github repo under the top level `blogs` folder. I created a new top level `journal` folder for the secondary blog. Idea is to dump any markdown file in that folder and have it served. To make it work, I updated the plugins section in my config file\\n```js title=\\"docusaurus.config.ts\\"\\n....\\n....\\n\\n  plugins: [\\n    [\\n      \'@docusaurus/plugin-content-blog\',\\n      {\\n        id: \'second-blog\',\\n        blogTitle: \'Journal\',\\n        blogDescription: \'Daily journal\',\\n        routeBasePath: \'journal\',\\n        path: \'../journal\',\\n        feedOptions: {\\n          type: [\'rss\', \'atom\'],\\n          title: \'nakam blog\',\\n          description: \'A daily journal from nakam blog\',\\n          copyright: \'nakam.org\',\\n          createFeedItems: async (params) => {\\n            const {blogPosts, defaultCreateFeedItems, ...rest} = params;\\n            return defaultCreateFeedItems({\\n              // keep only the 10 most recent blog posts in the feed\\n              blogPosts: blogPosts.filter((item, index) => index < 10),\\n              ...rest,\\n            });\\n          },\\n        },\\n      },\\n    ],\\n  ],\\n\\n....\\n....\\n```\\n\\nThat was it about adding a second blog. To make it easily accessable, I added another entry to the `navBar`\\n```js title=\\"docusaurus.config.ts\\"\\n  themeConfig: {\\n    navbar: {\\n      hideOnScroll: true,\\n      ....\\n      items: [\\n        {\\n          to: \'journal\',\\n          label: \'Journal\',\\n          position: \'left\'\\n        },\\n        {\\n          href: \'/rss.xml\',\\n          position: \'right\',\\n          className: \'feed-link\',\\n          \'aria-label\': \'rss Feed\',\\n        },\\n        {\\n          href: \'https://github.com/nakamorg\',\\n          position: \'right\',\\n          className: \'header-github-link\',\\n          \'aria-label\': \'GitHub repository\',\\n        },\\n      ],\\n    },\\n    ....\\n  }\\n```\\n\\nIf you noticed there are navigation items for feed and github as well, more on them later. Well, that was mostly it. While writing my first journal/daily blog, I realized that it\'s front-matter follows a pattern. I asked ChatGPT to generate me a bash script to create the blog template. This is what the script looks like\\n\\n```bash title=\\"daily-blog-creator.sh\\"\\n#!/bin/bash\\nset -eu\\n# Get the current date in the format yyyymmdd\\ncurrent_date=$(date +%Y%m%d)\\n\\n# Define the file name\\nfile_name=\\"journal/${current_date}-daily-journal.md\\"\\n\\n# Create the content\\ncontent=\\"---\\ntitle: $(date +%F) Daily Journal\\nslug: ${current_date}-daily-journal\\nauthors: [umesh]\\ntags:\\n- \'$(date +%Y-%m)\'\\n- \'$(date +%Y)\'\\n- journal\\n- daily\\nhide_table_of_contents: false\\n---\\nToday has been a please do something ab\x3c!-- truncate --\x3eout this and those.\\n\\"\\n\\necho \\"$content\\" > \\"$file_name\\"\\n```\\nRunning it would create a blog entry for that day. I used the `YYYY-MM` and `YYYY` tags - to make it easier to filter all the blogs from a particular year or month of the year.\\n\\nThat\'s the end of it. After all this was done I ran the script and wrote the first daily blog, you can find it at [/journal/2024/daily-journal](/journal/2024/daily-journal).\\n\\n## Other changes\\nI took some time today to fix some glaring issues with this blog. Like fixing the favicon and github, rss icons.\\n### Favicon\\nUsed [favicon-converter](https://favicon.io/favicon-converter/) on my profile image and generated the favicon images and copied those to `static/img` folder. One glaring issue down :relieved:\\n\\n### Github and Feed icons\\nThis is how the nav bar used to look before\\n![old nav bar](assets/navbar-20240808.png)\\nThe link for the rss feed of this blog and my github page looked so bad. I looked at how Docusaurus themselves have done it for their site and copied their config. So the navbar config looks like\\n```js title=\\"docusaurus.config.ts\\"\\n....\\n\\n      items: [\\n        {\\n          href: \'/rss.xml\',\\n          position: \'right\',\\n          className: \'feed-link\',\\n          \'aria-label\': \'rss Feed\',\\n        },\\n        {\\n          href: \'https://github.com/nakamorg\',\\n          position: \'right\',\\n          className: \'header-github-link\',\\n          \'aria-label\': \'GitHub repository\',\\n        },\\n      ],\\n\\n....\\n```\\ncorresponding css config\\n```css title=\\"src/css/custom.css\\"\\n....\\n\\n.header-github-link::before {\\n  content: \'\';\\n  width: 24px;\\n  height: 24px;\\n  display: flex;\\n  background-color: var(--ifm-navbar-link-color);\\n  mask-image: url(\\"data:image/svg+xml,%3Csvg viewBox=\'0 0 24 24\' xmlns=\'http://www.w3.org/2000/svg\'%3E%3Cpath d=\'M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12\'/%3E%3C/svg%3E\\");\\n  transition: background-color var(--ifm-transition-fast)\\n    var(--ifm-transition-timing-default);\\n}\\n\\n.header-github-link:hover::before {\\n  background-color: var(--ifm-navbar-link-hover-color);\\n}\\n\\n.feed-link::before {\\n  content: \'\';\\n  width: 24px;\\n  height: 24px;\\n  display: flex;\\n  background-color: var(--ifm-navbar-link-color);\\n  mask-image: url(\\"data:image/svg+xml,%3Csvg viewBox=\'0 0 24 24\' fill=\'none\' xmlns=\'http://www.w3.org/2000/svg\'%3E%3Cg id=\'SVGRepo_bgCarrier\' stroke-width=\'0\'%3E%3C/g%3E%3Cg id=\'SVGRepo_tracerCarrier\' stroke-linecap=\'round\' stroke-linejoin=\'round\'%3E%3C/g%3E%3Cg id=\'SVGRepo_iconCarrier\'%3E%3Cpath d=\'M7 18C7 18.5523 6.55228 19 6 19C5.44772 19 5 18.5523 5 18C5 17.4477 5.44772 17 6 17C6.55228 17 7 17.4477 7 18Z\' stroke=\'%23323232\' stroke-width=\'2\'%3E%3C/path%3E%3Cpath d=\'M11 19C11 15.6863 8.31371 13 5 13\' stroke=\'%23323232\' stroke-width=\'2\' stroke-linecap=\'round\'%3E%3C/path%3E%3Cpath d=\'M15 19C15 13.4772 10.5228 9 5 9\' stroke=\'%23323232\' stroke-width=\'2\' stroke-linecap=\'round\'%3E%3C/path%3E%3Cpath d=\'M19 19C19 11.268 12.732 5 5 5\' stroke=\'%23323232\' stroke-width=\'2\' stroke-linecap=\'round\'%3E%3C/path%3E%3C/g%3E%3C/svg%3E\\");\\n  transition: background-color var(--ifm-transition-fast)\\n    var(--ifm-transition-timing-default);\\n}\\n\\n.feed-link:hover::before {\\n  background-color: var(--ifm-navbar-link-hover-color);\\n}\\n\\n....\\n```\\nFeel free to copy it if you need :wink:. I got the Github icon svg from the Docusaurus github repo. As for the rss icon, I downloded the svg from [www.svgrepo.com](https://www.svgrepo.com/svg/507840/rss) and then converted it to css using [yoksel.github.io/url-encoder](https://yoksel.github.io/url-encoder/).\\n\\nThat\'s all for today and this post. Ah well, one more thing. This is how the navigation bar looks after these changes (in case the current nav-bar has changed after this blog entry).\\n![new nav bar](./assets/new-navbar-20240808.png)\\n\\nMuch better than before and I like it a lot as of now."},{"id":"2024/blogging-platform","metadata":{"permalink":"/blog/2024/blogging-platform","source":"@site/../blogs/2024-04-26-welcome.md","title":"Welcome","description":"I think I have settled on my blogging platform","date":"2024-04-26T00:00:00.000Z","tags":[{"inline":true,"label":"hello","permalink":"/blog/tags/hello"},{"inline":true,"label":"welcome","permalink":"/blog/tags/welcome"}],"readingTime":2.6,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Welcome","description":"I think I have settled on my blogging platform","slug":"2024/blogging-platform","tags":["hello","welcome"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Started a second blog","permalink":"/blog/2024/started-second-blog"}},"content":"For last couple of years, I have been looking for platforms for publishing. I looked at couple of places but couldn\'t find what I was looking for. Here\'s a very short\x3c!-- truncate --\x3e list of things I wanted for my setup\\n1. I fully own my content without the need for specialized import/export jobs.\\n1. I don\'t want to write CSS/HTML/JS myself. I would be happy writing simple txt files or use markdown files.\\n1. Content can be tracked for changes over time.\\n1. While I would certainly like many people to read what I write. I do not want big players suggesting my content to their \\"users\\". Information overload is a big issue.\\n1. Organic content discovery.\\n1. Mainly static. I do not want (anymore) comments or discussions on the platform itself. They can happen outside and I\'d be happy to edit the blog to link those back.\\n\\nOf course, I didn\'t come with these requirements from the very begining and they may change over time.\\n\\n# Setup\\nSo, what did I do to achieve what I wanted? \\n\\nI am in git and on Github, so using those for version controlling and storing the content made sense. I know if the content is on Github then I don\'t \\"own\\" it - they can pull the plug for some reason - but let\'s not get that cynical and I\'ll probably have the repo cloned on my local.\\n\\nFor building the website, I did an internet search for `how to build blogging website using markdown files`. Couple of results showed up. Two notable were [**mdbook**](https://rust-lang.github.io/mdBook) and [**Docusaurus**](https://docusaurus.io/). I settled with Docusaurus as it provided some nice features like: tagging, read time estimations, rss feed, nicer looking UI out of the box and most importantly I didn\'t have to spend lots of time fidgeting with how to use it. Though it requires adding some metadata(`front matter`) to the markdown files, I guess I can live with it as it could be easily searched for, removed or edited if needed. I tried mdBook as well, and it felt very slick and clean for what it does and it took me under 10 minutes to set up compared to about an hour and half on Docusaurus. But it would have stitched all the blogs into a single book - which might not feel natural given that I might end up writing on bunch of stuff - which might not necessarily fit into a single book category.\\n\\nGiven that I have the content on my local or on my Github account and its just some markdown files, there are a bunch of options for publishing like using AWS S3, Github Pages, my home-server (which I don\'t run 24/7). I went with github pages(with custom domain), as it was easy to use and it doesn\'t matter - I can change the publisher whenever I want.\\n\\nWell, I guess that is it. Welcome to this blog created with **Docusaurus** and currently being served from . . . maybe check the response headers and stuff because I don\'t know where it might end up six months from now."}]}}')}}]);