"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"2024/learnings-from-calbridge","metadata":{"permalink":"/blog/2024/learnings-from-calbridge","source":"@site/../blogs/2024-08-27-calbridge-learnings.md","title":"Learnings From Calbridge","description":"Development of calbridge is going on with full momentum now. I\'ve been spending hours working on it during nights. Just kidding. I mean I\'ve spent couple of nights working on it but then took some time off. And then wanted to write this blog before picking it up again. For the uninitiated, calbridge is a utility I\'ve been working on to integrate (or bridge) my Caldav server with my mail server. More background here, if you are interested.","date":"2024-08-27T00:00:00.000Z","tags":[{"inline":true,"label":"2024-08","permalink":"/blog/tags/2024-08"},{"inline":true,"label":"2024","permalink":"/blog/tags/2024"},{"inline":true,"label":"calbridge","permalink":"/blog/tags/calbridge"},{"inline":true,"label":"learning","permalink":"/blog/tags/learning"},{"inline":true,"label":"caldav","permalink":"/blog/tags/caldav"},{"inline":true,"label":"smtp","permalink":"/blog/tags/smtp"},{"inline":true,"label":"imap","permalink":"/blog/tags/imap"}],"readingTime":10.055,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Learnings From Calbridge","slug":"2024/learnings-from-calbridge","tags":["2024-08","2024","calbridge","learning","caldav","smtp","imap"]},"unlisted":false,"nextItem":{"title":"Basic Auth and Digest","permalink":"/blog/2024/basic-auth-and-digest"}},"content":"Development of [calbridge](https://github.com/nakamorg/calbridge) is going on with full momentum now. I\'ve been spending hours working on it during nights.\x3c!-- truncate --\x3e Just kidding. I mean I\'ve spent couple of nights working on it but then took some time off. And then wanted to write this blog before picking it up again. For the uninitiated, **calbridge** is a utility I\'ve been working on to integrate (or bridge) my Caldav server with my mail server. More background [here](/journal/2024/caldav-and-mail), if you are interested.\\n\\nAnyways, I wanted to take some time and jot down my thoughts and learnings from this project so far.\\n\\n## Using LLMs\\nI used `Claude Opus` to bootstrap the project. It provided me with the initial structure of the project. And then for every core functionality that I needed, it provided me with the starter code. I found it to be very helpful. It got me over the massive inertia of starting something from scratch and gave a boilerplate to start with. Although, the majority of the code it provided had to be re-written, it still helped a lot - as it gave pretty good hints on what libs to use and how to approach the problem and acted as a dummy duck or someone to yell on during debugging (trust me I swore a lot and it took it like a champ).\\n\\n## Auth\\nFirst thing that I came across and learned about was the `Digest Auth` and have already written a blog about that [here](/blog/2024/basic-auth-and-digest). I started with making simple `http` calls to the server to fetch my calendars and it seemed to have worked once the auth thing got fixed. But I soon found myself wanting more (like correctly parsing and handling calendar data). I was a bit hesitant to include too many dependencies to the project at first but ultimately gave up on that design goal as I wanted to have something working as soon as possible instead of spending too much time getting it perfect. Which brings us to our next topic. Packages.\\n\\n:::note\\nIt\'s okay to start with an \\"un-optimized\\" solution and have it working instead of spending too much energy building the perfect product in the first try. Once you have something working, you can iterate on it if you want.\\n:::\\n\\n## Packages\\nI looked for packages a lot. All I wanted were thin client libs for caldav, smtp and imap. But for caldav, at least, most of the libs were either abondoned or were providing both the server and the client. Ultimately, I settled with https://github.com/emersion - it came with webdav, caldav client and server (talking about thin clients). Thank you **emersion** for providing such a high quality implementation of these protocols. **emersion** provided all the libs I needed. While the libs were lacking in documentation and some working examples, they were implementing the target protocols (caldav, imap etc) to the point and using same terminology for variable, function names etc. as in the protocl spec. So, it didn\'t take much effort (if you don\'t consider 2-3 hours of fumbling around **much**) to write something working.\\n\\n:::note\\nAnd much of this time could have been saved if I had looked at github issues earlier.\\n:::\\n\\nNext, I\'ll discuss about some of these issues in detail.\\n\\n## golang http.request - Cannot reuse\\nThe caldav lib I used provided a neat way of passing in a custom HTTPClient. This way I could implemet a custom `digest auth` client to authenticate with my server. A working implementation for **digest auth** need to make at least two calls to the server. First a dummy call, just to get the authorization challenge from the server and then the actual call with the authorization header set. I reused the same `http.Request` in both calls.\\n\\nThe second call silently failed. The returned `http.Response` was `nil` without any errors. I thought there was something wrong with the client, my digest auth implementation or the caldav server. I inserted a bunch of `Print` statements to debug it (now you know what I use for debugging). And then added some more **print** statements and realized that `Response` is always `nil`. I had no idea what might have caused it. Out of desperation, I passed the function context to my GPT and asked it why the response is nil all the time. It suggested not to re-use the previous `response` object. I told it not to be stupid. It apologized and said the the same request shouldn\'t be re-used. That made some sense to me and I made a clone of the previous request with `req.Clone(context.Background())` but it still didn\'t work though there were some error response from the server this time. Some progress, eh? Server complained that I were not asking for any data. I printed the original request and could see that I was indeed asking for some data. Then I printed the cloned request and noticed that it didn\'t have the `body`. Hmm, what kind of `clone` was it?\\n\\nTurns out that the `req.Clone` doesn\'t clone the request body. Here\'s the comment for that method\\n> Clone returns a deep copy of r with its context changed to ctx. The provided ctx must be non-nil.\\n>\\n> For an outgoing client request, the context controls the entire lifetime of a request and its response: obtaining a connection, sending the request, and reading the response headers and body.\\n\\nNowhere does it mentions that it won\'t clone the body. It even says that it makes a deep copy. But the entire game is of the `context`. Once the context is done, it seems the Body is done for or consumed. So, I needed to make a copy of the body before sending out the request and used that copy to set the body of the cloned request. `req.GetBody()` helped in making the copy of the request.\\n\\nAnd that was it. The server started returning my calendars after that.\\n\\n:::note\\nThat\'s not entirely true. During debugging, I thought that maybe I\'m not sending the correct queries to the server. So I tried with bunch of different queries and settings. And then spent some extra minutes to realize that I\'ve messed up the queries and then some more to correcting those.\\n:::\\n\\n## SMPT - Just use an external package\\nI started with `net/smtp` package that is included with the standard Golang installation. There was nothing wrong with it but it seems that its development is frozen and the authors suggested using some other maintained lib. So, after writing a working implementation using this \\"obselete\\" lib, I searched for another lib and then re-wrote the thing to make it compatible with the new lib.\\n\\nOne thing I solidified during this experience was to provide a good interface to your users. This allows to change your implementation as you want (change libs/packages or re-write whole thing by youself) as long as you don\'t alter the user interface. I started with `NewSMTPClient` function to return a custom (hidden) client object and a `SendCalendarInvite(calObject caldav.CalendarObject)` method for the client. The client didn\'t expose its fields directly to the end user - so it was pretty easy to switch to another package for my `SendCalendarInvite` implementation without making any changes to the front-end.\\n\\n## IMAP - Sweet time that we spent together\\nI spent about 90 minutes on a very \\"stupid\\" mistake. Here\'s how. So I needed to ask my mail server for all the emails from last few hours. Then I could process them to see if they had any calendar invite. `github.com/emersion/go-imap` lib along with `github.com/emersion/go-message/mail` make this whole thing a no-brainer given that you have some brain cells to correctly use these packages or you are willing to spend some time looking at protocol RFCs or maybe just search Github issues for the problems you\'r facing.\\n\\nHere\'s what happened. I managed to find the sequence numbers of the emails that I needed. But didn\'t check if the returned result was empty or not. Sending the empty sequence numbers to server to fetch those results returned weird errors from the server which gave no hint about the actual issue.\\n```go\\ncriteria.SentSince = time.Now().Add(-4*hours)\\nseqNums, err := c.Search(criteria)\\n\\nitems := []imap.FetchItem{imap.FetchBody}\\nmsgs := make(chan *imap.Message, len(seqNums))\\nseqSet := new(imap.SeqSet)\\nseqSet.AddNum(seqNums...)\\nif err := c.Fetch(seqSet, items, msgs); err != nil {\\n    return nil, fmt.Errorf(\\"failed to fetch email: %v\\", err)\\n}\\n```\\n\\nSo, the first thing that needed to be done was not to query the server if you didn\'t have anything to query for.\\n```go\\nseqNums, err := c.Search(criteria)\\n\\nif len(seqNums) == 0 {\\n    return []string{}, nil\\n}\\n```\\n\\nAlright, that makes sense now. So I increased my search criteria to 10 hours and was pretty certain that I had several mails in that window. And that resulted in segmentation fault. Meaning I was trying to read some memory location which I had no business reading. Fortunately, unlike `c`, golang gives you some hints about the crime location. It pointed out to me the line number where I was trying to read the mail body.\\n\\nLet\'s look at the code again (with just the necessary parts)\\n```go\\nitems := []imap.FetchItem{imap.FetchBody}\\nmsgs := make(chan *imap.Message, len(seqNums))\\nseqSet := new(imap.SeqSet)\\nseqSet.AddNum(seqNums...)\\nif err := c.Fetch(seqSet, items, msgs); err != nil {\\n    return nil, fmt.Errorf(\\"failed to fetch email: %v\\", err)\\n}\\n```\\n`items` that I want to fetch is the email body (`imap.FetchBody` here is the string `BODY`) - as that\'s the thing that would have the calendar invites I\'m after. But the body field of all the emails that it fetched was always `nil`. It was weird. I had explicitly specified to fetch the `BODY` and the body was nil. Then my monkey brain told me not to optimize and just fetch everything. Conveniently, there was `imap.FetchAll` and I thought it would fetch everything. But NO. Both of these options fetched everthing but the email body. After plucking another 100 hair from my head and I surrendered and looked online. Within a minute or so, I came across this github comment `https://github.com/emersion/go-imap/issues/306#issuecomment-546532174` - turned out I had to fetch `BODY.PEEK[]`. Fetch `ALL` or `BODY` do not fetch everthing or the body respectively (as one would have expected) but the headers and stuff. So, folks please read those RFCs.\\n\\n:::note\\nPeek keeps the email unread, so one can use `BODY[]` as well if they want to mark the mail read. (Look at the smart me)\\n:::\\n\\nSo, here\'s the wokring code\\n```go\\nitems := []imap.FetchItem{imap.FetchItem(\\"BODY.PEEK[]\\")}\\n.. removed for brevity ..\\nif err := c.Fetch(seqSet, items, msgs); err != nil {\\n    return nil, fmt.Errorf(\\"failed to fetch email: %v\\", err)\\n}\\n```\\n\\nFunnily enough, when I started working on the IMAP functionlity. I asked `Claude Opus` (a gpt) to write me the code, it suggested the folowing\\n```go\\nsection := &imap.BodySectionName{}\\nitems := []imap.FetchItem{section.FetchItem()}\\n```\\n\\nwhich translates to a working approach\\n```go\\nitems := []imap.FetchItem{imap.FetchItem(\\"BODY[]\\")}\\n```\\nBut I became a smart-ass and asked why it was making it so complicated. We needed just the body and `FetchBody` provided that, so why couldn\'t we use that instead? The GPT, like it always does, apologized to me, praised me for being so smart, and suggested that we could indeed just use `FetchBody`. Sometimes I think it did that on purpose, maybe to hold some grudge or something /s.\\n\\n## Something to end the blog with\\nLike I mentioned in the begining, I wanted to keep the final binary very small and might end up re-writing some of the functionalities that the packages provide by myself. As of now, I think that the webdav dependency (it provides caldav) can be easily do away with. As it just needs to make some simple http calls to the server. So that\'s something I\'ve been looking towards getting rid of. But not until I have a fully working solution ready first.\\n\\n:::note\\nI loved using these **note** sections in this blog entry. Sometimes at places where it didn\'t make any sense. And now this blog entry being done, maybe I can go back actually implementaing the real thing.\\n:::"},{"id":"2024/basic-auth-and-digest","metadata":{"permalink":"/blog/2024/basic-auth-and-digest","source":"@site/../blogs/2024-08-22-digest-auth.md","title":"Basic Auth and Digest","description":"I\'ve started working on calbridge and spent some time working on the caldav client part of it. While connecting to the caldav server, I passed in the creds as basic auth. The server repeatedly returned 401 - Unauthorized. I might have spent roughly an hour figuring out where I might be sending the wrong username, password or the server url.","date":"2024-08-22T00:00:00.000Z","tags":[{"inline":true,"label":"2024-08","permalink":"/blog/tags/2024-08"},{"inline":true,"label":"2024","permalink":"/blog/tags/2024"},{"inline":true,"label":"digest","permalink":"/blog/tags/digest"},{"inline":true,"label":"basic auth","permalink":"/blog/tags/basic-auth"}],"readingTime":3.675,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Basic Auth and Digest","slug":"2024/basic-auth-and-digest","tags":["2024-08","2024","digest","basic auth"]},"unlisted":false,"prevItem":{"title":"Learnings From Calbridge","permalink":"/blog/2024/learnings-from-calbridge"},"nextItem":{"title":"Terraform Provider Dynamic Configuration","permalink":"/blog/2024/terraform-provider-dynamic-configuration"}},"content":"I\'ve started working on [calbridge](/journal/2024/caldav-and-mail) and spent some time working on the caldav client part of it.\x3c!-- truncate --\x3e While connecting to the caldav server, I passed in the creds as basic auth. The server repeatedly returned `401 - Unauthorized`. I might have spent roughly an hour figuring out where I might be sending the wrong username, password or the server url.\\n\\nAnd then by luck or whatever, I stumbled across this part of the response\\n```xml\\n<?xml version=\\"1.0\\" encoding=\\"utf-8\\"?>\\n<d:error xmlns:d=\\"DAV:\\" xmlns:s=\\"http://sabredav.org/ns\\">\\n  <s:sabredav-version>4.6.0</s:sabredav-version>\\n  <s:exception>Sabre\\\\DAV\\\\Exception\\\\NotAuthenticated</s:exception>\\n  <s:message>No \'Authorization: Digest\' header found. Either the client didn\'t send one, or the server is misconfigured</s:message>\\n</d:error>\\n```\\n**No \'Authorization: Digest\' header found** - the poor fella was expecting the `Authorization: Digest` and I was passing in the `Authorization: Basic`.\\n\\n:::tip\\nIf you are using curl, pass the `--digest` flag and it will handle the **digest** auth flow for you. \\n:::\\n\\nI\'ve heard of this auth mechanism before but never went into much details of what it is, why it is and how it is. These three questions I\'ll try to cover in this blog. Maybe not in much details but something to keep it interesting and informative\\n\\n## What\\nSo, it\'s a HTTP auth mechanism like basic auth. But unlike basic auth, where username and password are transmitted in plain-text and can be intercepted and decoded by anyone on the network (if you are using HTTP (not HTTPS)); with auth digest you never send the username and password over. Instead you use your creds to solve a puzzle that the server sents you. Now, of course, if you are not on HTTPS - an attacker on the network can easily intercept this solution to the puzzle and then impersonate you. But at least they won\'t have your password.\\n\\nMoral of the story: Always use a secured connection (HTTPS) if you are sending over sensitive information.\\n\\n## Why\\nI think the last section kinda answered this question. We needed it to avoid sending our passwords in plain-text over an HTTP connection.\\n:::info\\nA question for you: What do you think is the best way to create a user, in the first place, over such a connection?\\n:::\\n\\n## How\\nI\'ll try to gloss over most of the technical details but still keep enough to satiate those of you who dig these kind of details. I\'ll use some Golang code to show the implementation details\\n1. Client asks the server for a protected resource (without sending any creds)\\n    ```go\\n    req, _ := http.NewRequest(\\"GET\\", URL, nil)\\n    resp, _ := http.DefaultClient.Do(req)\\n    ```\\n2. Server responds with `StatusUnauthorized (401)` status code and sends the challenge in `WWW-Authenticate` header\\n    ```go\\n    if resp.StatusCode == http.StatusUnauthorized {\\n        challenge := resp.Header.Get(\\"WWW-Authenticate\\")\\n\\n        req, _ = http.NewRequest(\\"GET\\", URL, nil)\\n\\n        // Set the Digest authentication header\\n        req.Header.Set(\\"Authorization\\", getDigestAuthorization(challenge, Username, Password, \\"GET\\", URL))\\n\\n        // Send the request with Digest authentication\\n        resp, _ = http.DefaultClient.Do(req)\\n    }\\n    ```\\n3. In `WWW-Authenticate`, server sends couple of fields like `realm`, `nonce` and these are used together with username and password to generate a hash. This hash is what is sent back to the server in `Authorization` header\\n    ```go\\n    func getDigestAuthorization(challenge, username, password, method, uri string) string {\\n        // Parse the Digest challenge\\n        fields := parseDigestChallenge(challenge)\\n\\n        realm := fields[\\"realm\\"]\\n        nonce := fields[\\"nonce\\"]\\n\\n        // Generate the response hash\\n        ha1 := getMD5(fmt.Sprintf(\\"%s:%s:%s\\", username, realm, password))\\n        ha2 := getMD5(fmt.Sprintf(\\"%s:%s\\", method, uri))\\n        response := getMD5(fmt.Sprintf(\\"%s:%s:%s\\", ha1, nonce, ha2))\\n\\n        // Construct the Digest authorization header\\n        authParams := fmt.Sprintf(\\n            `username=\\"%s\\", realm=\\"%s\\", nonce=\\"%s\\", uri=\\"%s\\", response=\\"%s\\"`,\\n            username, realm, nonce, uri, response\\n        )\\n        return \\"Digest \\" + authParams\\n    }\\n    ```\\n4. As you might have noticed, we send back the username, realm, nonce and the hash to the server but not the password. Using the username, the server looks up the password from its db and does this calculation on its side. If the hash matches to the server calculated hash then the request is authenticated otherwise not.\\n\\nThat was it about the digest auth. A clever way if proving you are who you say you are without sending over your password. Again it might not be necessary if you use HTTPS and might not be very secure(citations needed) but it\'s still much better than basic auth.\\n\\n:::warning\\nI\'ve removed lots of implementation details from the code snippets above. Do not copy paste this code for your implementation of digest auth. Use some existing library or follow the official RFC or spec of the protocol.\\n:::"},{"id":"2024/terraform-provider-dynamic-configuration","metadata":{"permalink":"/blog/2024/terraform-provider-dynamic-configuration","source":"@site/../blogs/2024-08-20-terraform-providers.md","title":"Terraform Provider Dynamic Configuration","description":"I had a requirement where I wanted to work with Honeycombio\'s terraform provider. Unlike Datadog, Honeycomb has the concept of environments. It maps perfectly with our infra environments, i.e we can send prd telemetry to prd honeycomb environment and dev to dev.","date":"2024-08-20T00:00:00.000Z","tags":[{"inline":true,"label":"2024-08","permalink":"/blog/tags/2024-08"},{"inline":true,"label":"2024","permalink":"/blog/tags/2024"},{"inline":true,"label":"terraform","permalink":"/blog/tags/terraform"},{"inline":true,"label":"provider","permalink":"/blog/tags/provider"},{"inline":true,"label":"honeycomb","permalink":"/blog/tags/honeycomb"}],"readingTime":2.085,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Terraform Provider Dynamic Configuration","slug":"2024/terraform-provider-dynamic-configuration","tags":["2024-08","2024","terraform","provider","honeycomb"]},"unlisted":false,"prevItem":{"title":"Basic Auth and Digest","permalink":"/blog/2024/basic-auth-and-digest"},"nextItem":{"title":"Github Actions for the Blog","permalink":"/blog/2024/github-actions"}},"content":"I had a requirement where I wanted to work with Honeycombio\'s terraform provider. Unlike Datadog, Honeycomb has the concept of environments. It maps perfectly with our infra\x3c!-- truncate --\x3e environments, i.e we can send prd telemetry to prd honeycomb environment and dev to dev.\\n\\nHoneycomb provider uses API keys for configuration and each environment has its own keys. That meant that whenever we wanted to run our terraform code, we needed to set some env vars to specify the correct environment\'s api key. This gets particularly messy on our CI server, as we have a single instance to handle all our environments. When we ran a CI pipeline, there definetely is a way to specify which env it should target and we can very easily use that information in our terraform code. But terraform doesn\'t provide a native way to read any env vars. It reads `TF_VARS_xx` and target providers can read whetever env vars they expect to configure themselves. In case of Honeycomb, it\'s `HONEYCOMB_API_KEY`. But we can\'t mutate this nev var on CI server every time we run a job/pipeline, as there might be multiple jobs, simultaneoulys taregting both the dev and prd environments.\\n\\n## Solution\\nWe decided to store the API keys in AWS Secrets Manager. Our dev aws account stores the key for dev honeycomb env and prd for prd. We already have tooling in place to pass in the env specific AWS role when running the pipeline, so the terraform aws provider would be configured properly. \\n\\n:::note\\nWe could, of couse, modify the tooling to support this Honeycomb case in a similar way. But that didn\'t sound like a good idea. As it sets the precedent of doing this for all the providers we use or will use.\\n:::\\n\\nSo, the solution is simple. We use aws provider to fetch the target account\'s API key and use those to configure the honey-comb provider. Here\'s the code snippet\\n\\n```json title=\\"provider.tf\\"\\nterraform {\\n  required_providers {\\n    honeycombio = {\\n      source  = \\"honeycombio/honeycombio\\"\\n      version = \\"~> 0.26.0\\"\\n    }\\n    aws = {\\n        source = \\"hashicorp/aws\\"\\n    }\\n  }\\n}\\n\\ndata \\"aws_secretsmanager_secret\\" \\"honeycomb\\" {\\n  name = \\"<name of aws secret holding honeycomb api key>\\"\\n}\\n\\ndata \\"aws_secretsmanager_secret_version\\" \\"honeycomb\\" {\\n  secret_id = data.aws_secretsmanager_secret.honeycomb.id\\n}\\n\\nlocals {\\n  honeycomb_configuration_key = try(jsondecode(data.aws_secretsmanager_secret_version.honeycomb.secret_string)[\\"configuration_key\\"], null)\\n}\\n\\nprovider \\"honeycombio\\" {\\n  api_key = local.honeycomb_configuration_key\\n}\\n```\\n\\nThe same idea can be extended to other similar providers or to specify different configuration depending on the environemt, e.g: dev key with limited access and prd key with full access."},{"id":"2024/github-actions","metadata":{"permalink":"/blog/2024/github-actions","source":"@site/../blogs/2024-08-16-github-actions.md","title":"Github Actions for the Blog","description":"Thinking about adding github actions to build and deploy this blog instead of running them on a VM on my machine. For some reason I couldn\'t get myself to install nodejs and npm on my regular workstation. Maybe I should do that as well.","date":"2024-08-16T00:00:00.000Z","tags":[{"inline":true,"label":"2024-08","permalink":"/blog/tags/2024-08"},{"inline":true,"label":"2024","permalink":"/blog/tags/2024"},{"inline":true,"label":"github action","permalink":"/blog/tags/github-action"}],"readingTime":1.89,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Github Actions for the Blog","slug":"2024/github-actions","tags":["2024-08","2024","github action"],"hide_table_of_contents":true},"unlisted":false,"prevItem":{"title":"Terraform Provider Dynamic Configuration","permalink":"/blog/2024/terraform-provider-dynamic-configuration"},"nextItem":{"title":"Reading List","permalink":"/blog/2024/reading-list"}},"content":"Thinking about adding github actions to build and deploy this blog instead of running them on a VM on my machine. For some reason\x3c!-- truncate --\x3e I couldn\'t get myself to install nodejs and npm on my regular workstation. Maybe I should do that as well.\\n\\nBut for the time being I\'m going to try my luck with github actions. Just added a github workflow file and pushing this blog file to master should trigger that. So, let get triggering!\\n\\nSo, that worked. But only halfway. The last step `npm run deploy` failed asking for my github username or specifying using ssh key. I was hoping that it would know how to push the changes as it was already running in github action. Let\'s see if `Claude Opus` can help us figure this out. After multiple back and forth it suggested to use this snippet\\n```yaml\\n- name: Deploy\\n    run: |\\n    npm run deploy\\n    working-directory: docusaurus\\n    env:\\n    GIT_USER: github-actions\\n    GIT_PASS: ${{ secrets.GITHUB_TOKEN }}\\n```\\nI think it should work. I\'m just not confident about the `GIT_USER` part. Should I use my github username there instead? Anyways, let\'s try pushing and see how it goes.\\n\\nThe action run was succesful but it still didn\'t deploy, complaining about setting git user email and stuff. So, let\'s try with\\n\\n```yaml\\n- name: Deploy\\n  run: |\\n    git config --global user.email \\"${{ github.actor }}@users.noreply.github.com\\"\\n    git config --global user.name \\"${{ github.actor }}\\"\\n    npm run deploy\\n  working-directory: docusaurus\\n  env:\\n    GIT_USER: ${{ github.actor }}\\n    GIT_PASS: ${{ secrets.GITHUB_TOKEN }}\\n```\\n\\nThat was it. The blog is \\"on\\" now. All I have to do is write and push to master and github will take care of build, deploy and hosting. Good times. You can find the complete and up-to-date code in github workflows of [this repo](https://github.com/nakamorg/nakamorg.github.io). One more thing though, I\'m not sure if the caching for node modules is working. Initial runs of `npm ci` took about 10 secs and the latest one with a cache hit took 6 secs - so maybe it\'s working? Maybe I can try doing `ls node_modules` in the github action and see or maybe there are other better options? But let\'s do that some other time. It\'s almost 19:18 and I need to get off working and start my work out."},{"id":"2024/reading-list","metadata":{"permalink":"/blog/2024/reading-list","source":"@site/../blogs/2024-08-15-reading-list.md","title":"Reading List","description":"I\'ve been thinking about managing a reading list, basically a list of urls or excerpts as I read them or get to know about them. Previously, I set up Linkwarden for this but didn\'t use it much (or at all). Hoping that this rlog (running blog) can substitute that. Hopefully, I\'ll keep it updated with everything insteresting I read on day to day basis, but it\'ll mostly be media that I\'d consume on my workstation (same machine I use to write this blog). Anyways, something is better than nothing. off we go","date":"2024-08-15T00:00:00.000Z","tags":[{"inline":true,"label":"reading","permalink":"/blog/tags/reading"},{"inline":true,"label":"reading-list","permalink":"/blog/tags/reading-list"}],"readingTime":1.15,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Reading List","slug":"2024/reading-list","tags":["reading","reading-list"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Github Actions for the Blog","permalink":"/blog/2024/github-actions"},"nextItem":{"title":"Started a second blog","permalink":"/blog/2024/started-second-blog"}},"content":"I\'ve been thinking about managing a reading list, basically a list of urls or excerpts as I read them or get to know about them. Previously, I set up Linkwarden\x3c!-- truncate --\x3e for this but didn\'t use it much (or at all). Hoping that this rlog (running blog) can substitute that. Hopefully, I\'ll keep it updated with everything insteresting I read on day to day basis, but it\'ll mostly be media that I\'d consume on my workstation (same machine I use to write this blog). Anyways, something is better than nothing. off we go\\n\\n## 2024-08-15\\n15th August - Independence Day, India\\n- [medium.com/yandex/good-retry-bad-retry-an-incident-story](https://medium.com/yandex/good-retry-bad-retry-an-incident-story-648072d3cee6) - Very interesting and fun read. Goes into the details of when and why `retries` should be used. How simple or simple exponetial backoff retries could be major issue when is system is recovering from downtime. Sugeests to use retries with `Retry Budget`, `Circuit Breaker`, `Load Shedding`, `Deadline Propogation` etc. Former two are implemented on client whereas later two on servers.\\n\\n## 2024-08-16\\n- [isovalent.com/blog/post/demystifying-cni](https://isovalent.com/blog/post/demystifying-cni) - A short article briefly discusses the container runtime and network interface and goes on to build a CNI plugin using bash. Apparently, you only need two files. A conf file at `/etc/cni/net.d/` and a CNI executable at `/opt/cni/bin/`. CRI loads the first conf file and finds the corresponding executable and then calls the CNI executable as per the CNI spec."},{"id":"2024/started-second-blog","metadata":{"permalink":"/blog/2024/started-second-blog","source":"@site/../blogs/2024-08-08-second-blog.md","title":"Started a second blog","description":"For some time, I wanted to start with a daily blog. Yes, I know I started this blog about four months ago and have only two entries as of now (including this one). Guess I\'m mostly into wasting my time finding which blog genertors to use and starting up a blog site but not into writing. Anyways, I wanted a new blog where I could write freely, without going into much details of the things, about my daily life and routine. I\'ve set up a docusaurus blog previously (the one you are currently reading). I spent some time checking if the mdbook would be a good fit for this daily journal blog. Fired up a test mdbook book and compared it with docusaurus style blog I had. Docusaurus style felt good to eyes (it looked nice) - so I decided to stick with docusaurus and started a second blog on same docusaurus instance. You can find it at /journal address of this site.","date":"2024-08-08T00:00:00.000Z","tags":[{"inline":true,"label":"2024-08","permalink":"/blog/tags/2024-08"},{"inline":true,"label":"2024","permalink":"/blog/tags/2024"},{"inline":true,"label":"journal","permalink":"/blog/tags/journal"},{"inline":true,"label":"daily","permalink":"/blog/tags/daily"},{"inline":true,"label":"multiple blogs","permalink":"/blog/tags/multiple-blogs"},{"inline":true,"label":"rss","permalink":"/blog/tags/rss"},{"inline":true,"label":"custom css","permalink":"/blog/tags/custom-css"}],"readingTime":4.855,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Started a second blog","slug":"2024/started-second-blog","tags":["2024-08","2024","journal","daily","multiple blogs","rss","custom css"]},"unlisted":false,"prevItem":{"title":"Reading List","permalink":"/blog/2024/reading-list"},"nextItem":{"title":"Welcome","permalink":"/blog/2024/blogging-platform"}},"content":"For some time, I wanted to start with a daily blog. Yes, I know I started this blog about four months ago and have only two entries as of now (including this one). Guess I\'m mostly into wasting\x3c!-- truncate --\x3e my time finding which blog genertors to use and starting up a blog site but not into writing. Anyways, I wanted a new blog where I could write freely, without going into much details of the things, about my daily life and routine. I\'ve set up a docusaurus blog previously (the one you are currently reading). I spent some time checking if the **mdbook** would be a good fit for this daily journal blog. Fired up a test mdbook book and compared it with docusaurus style blog I had. Docusaurus style felt good to eyes (it looked nice) - so I decided to stick with docusaurus and started a second blog on same docusaurus instance. You can find it at [/journal](/journal) address of this site.\\n\\n## Detail about the setup\\nThis blog lives at [nakamorg.github.io](https://github.com/nakamorg/nakamorg.github.io) github repo under the top level `blogs` folder. I created a new top level `journal` folder for the secondary blog. Idea is to dump any markdown file in that folder and have it served. To make it work, I updated the plugins section in my config file\\n```js title=\\"docusaurus.config.ts\\"\\n....\\n....\\n\\n  plugins: [\\n    [\\n      \'@docusaurus/plugin-content-blog\',\\n      {\\n        id: \'second-blog\',\\n        blogTitle: \'Journal\',\\n        blogDescription: \'Daily journal\',\\n        routeBasePath: \'journal\',\\n        path: \'../journal\',\\n        feedOptions: {\\n          type: [\'rss\', \'atom\'],\\n          title: \'nakam blog\',\\n          description: \'A daily journal from nakam blog\',\\n          copyright: \'nakam.org\',\\n          createFeedItems: async (params) => {\\n            const {blogPosts, defaultCreateFeedItems, ...rest} = params;\\n            return defaultCreateFeedItems({\\n              // keep only the 10 most recent blog posts in the feed\\n              blogPosts: blogPosts.filter((item, index) => index < 10),\\n              ...rest,\\n            });\\n          },\\n        },\\n      },\\n    ],\\n  ],\\n\\n....\\n....\\n```\\n\\nThat was it about adding a second blog. To make it easily accessable, I added another entry to the `navBar`\\n```js title=\\"docusaurus.config.ts\\"\\n  themeConfig: {\\n    navbar: {\\n      hideOnScroll: true,\\n      ....\\n      items: [\\n        {\\n          to: \'journal\',\\n          label: \'Journal\',\\n          position: \'left\'\\n        },\\n        {\\n          href: \'/rss.xml\',\\n          position: \'right\',\\n          className: \'feed-link\',\\n          \'aria-label\': \'rss Feed\',\\n        },\\n        {\\n          href: \'https://github.com/nakamorg\',\\n          position: \'right\',\\n          className: \'header-github-link\',\\n          \'aria-label\': \'GitHub repository\',\\n        },\\n      ],\\n    },\\n    ....\\n  }\\n```\\n\\nIf you noticed there are navigation items for feed and github as well, more on them later. Well, that was mostly it. While writing my first journal/daily blog, I realized that it\'s front-matter follows a pattern. I asked ChatGPT to generate me a bash script to create the blog template. This is what the script looks like\\n\\n```bash title=\\"daily-blog-creator.sh\\"\\n#!/bin/bash\\nset -eu\\n# Get the current date in the format yyyymmdd\\ncurrent_date=$(date +%Y%m%d)\\n\\n# Define the file name\\nfile_name=\\"journal/${current_date}-daily-journal.md\\"\\n\\n# Create the content\\ncontent=\\"---\\ntitle: $(date +%F) Daily Journal\\nslug: ${current_date}-daily-journal\\nauthors: [umesh]\\ntags:\\n- \'$(date +%Y-%m)\'\\n- \'$(date +%Y)\'\\n- journal\\n- daily\\nhide_table_of_contents: false\\n---\\nToday has been a please do something ab\x3c!-- truncate --\x3eout this and those.\\n\\"\\n\\necho \\"$content\\" > \\"$file_name\\"\\n```\\nRunning it would create a blog entry for that day. I used the `YYYY-MM` and `YYYY` tags - to make it easier to filter all the blogs from a particular year or month of the year.\\n\\nThat\'s the end of it. After all this was done I ran the script and wrote the first daily blog, you can find it at [/journal/2024/daily-journal](/journal/2024/daily-journal).\\n\\n## Other changes\\nI took some time today to fix some glaring issues with this blog. Like fixing the favicon and github, rss icons.\\n### Favicon\\nUsed [favicon-converter](https://favicon.io/favicon-converter/) on my profile image and generated the favicon images and copied those to `static/img` folder. One glaring issue down :relieved:\\n\\n### Github and Feed icons\\nThis is how the nav bar used to look before\\n![old nav bar](assets/navbar-20240808.png)\\nThe link for the rss feed of this blog and my github page looked so bad. I looked at how Docusaurus themselves have done it for their site and copied their config. So the navbar config looks like\\n```js title=\\"docusaurus.config.ts\\"\\n....\\n\\n      items: [\\n        {\\n          href: \'/rss.xml\',\\n          position: \'right\',\\n          className: \'feed-link\',\\n          \'aria-label\': \'rss Feed\',\\n        },\\n        {\\n          href: \'https://github.com/nakamorg\',\\n          position: \'right\',\\n          className: \'header-github-link\',\\n          \'aria-label\': \'GitHub repository\',\\n        },\\n      ],\\n\\n....\\n```\\ncorresponding css config\\n```css title=\\"src/css/custom.css\\"\\n....\\n\\n.header-github-link::before {\\n  content: \'\';\\n  width: 24px;\\n  height: 24px;\\n  display: flex;\\n  background-color: var(--ifm-navbar-link-color);\\n  mask-image: url(\\"data:image/svg+xml,%3Csvg viewBox=\'0 0 24 24\' xmlns=\'http://www.w3.org/2000/svg\'%3E%3Cpath d=\'M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12\'/%3E%3C/svg%3E\\");\\n  transition: background-color var(--ifm-transition-fast)\\n    var(--ifm-transition-timing-default);\\n}\\n\\n.header-github-link:hover::before {\\n  background-color: var(--ifm-navbar-link-hover-color);\\n}\\n\\n.feed-link::before {\\n  content: \'\';\\n  width: 24px;\\n  height: 24px;\\n  display: flex;\\n  background-color: var(--ifm-navbar-link-color);\\n  mask-image: url(\\"data:image/svg+xml,%3Csvg viewBox=\'0 0 24 24\' fill=\'none\' xmlns=\'http://www.w3.org/2000/svg\'%3E%3Cg id=\'SVGRepo_bgCarrier\' stroke-width=\'0\'%3E%3C/g%3E%3Cg id=\'SVGRepo_tracerCarrier\' stroke-linecap=\'round\' stroke-linejoin=\'round\'%3E%3C/g%3E%3Cg id=\'SVGRepo_iconCarrier\'%3E%3Cpath d=\'M7 18C7 18.5523 6.55228 19 6 19C5.44772 19 5 18.5523 5 18C5 17.4477 5.44772 17 6 17C6.55228 17 7 17.4477 7 18Z\' stroke=\'%23323232\' stroke-width=\'2\'%3E%3C/path%3E%3Cpath d=\'M11 19C11 15.6863 8.31371 13 5 13\' stroke=\'%23323232\' stroke-width=\'2\' stroke-linecap=\'round\'%3E%3C/path%3E%3Cpath d=\'M15 19C15 13.4772 10.5228 9 5 9\' stroke=\'%23323232\' stroke-width=\'2\' stroke-linecap=\'round\'%3E%3C/path%3E%3Cpath d=\'M19 19C19 11.268 12.732 5 5 5\' stroke=\'%23323232\' stroke-width=\'2\' stroke-linecap=\'round\'%3E%3C/path%3E%3C/g%3E%3C/svg%3E\\");\\n  transition: background-color var(--ifm-transition-fast)\\n    var(--ifm-transition-timing-default);\\n}\\n\\n.feed-link:hover::before {\\n  background-color: var(--ifm-navbar-link-hover-color);\\n}\\n\\n....\\n```\\nFeel free to copy it if you need :wink:. I got the Github icon svg from the Docusaurus github repo. As for the rss icon, I downloded the svg from [www.svgrepo.com](https://www.svgrepo.com/svg/507840/rss) and then converted it to css using [yoksel.github.io/url-encoder](https://yoksel.github.io/url-encoder/).\\n\\nThat\'s all for today and this post. Ah well, one more thing. This is how the navigation bar looks after these changes (in case the current nav-bar has changed after this blog entry).\\n![new nav bar](./assets/new-navbar-20240808.png)\\n\\nMuch better than before and I like it a lot as of now."},{"id":"2024/blogging-platform","metadata":{"permalink":"/blog/2024/blogging-platform","source":"@site/../blogs/2024-04-26-welcome.md","title":"Welcome","description":"I think I have settled on my blogging platform","date":"2024-04-26T00:00:00.000Z","tags":[{"inline":true,"label":"hello","permalink":"/blog/tags/hello"},{"inline":true,"label":"welcome","permalink":"/blog/tags/welcome"}],"readingTime":2.6,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Welcome","description":"I think I have settled on my blogging platform","slug":"2024/blogging-platform","tags":["hello","welcome"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Started a second blog","permalink":"/blog/2024/started-second-blog"}},"content":"For last couple of years, I have been looking for platforms for publishing. I looked at couple of places but couldn\'t find what I was looking for. Here\'s a very short\x3c!-- truncate --\x3e list of things I wanted for my setup\\n1. I fully own my content without the need for specialized import/export jobs.\\n1. I don\'t want to write CSS/HTML/JS myself. I would be happy writing simple txt files or use markdown files.\\n1. Content can be tracked for changes over time.\\n1. While I would certainly like many people to read what I write. I do not want big players suggesting my content to their \\"users\\". Information overload is a big issue.\\n1. Organic content discovery.\\n1. Mainly static. I do not want (anymore) comments or discussions on the platform itself. They can happen outside and I\'d be happy to edit the blog to link those back.\\n\\nOf course, I didn\'t come with these requirements from the very begining and they may change over time.\\n\\n# Setup\\nSo, what did I do to achieve what I wanted? \\n\\nI am in git and on Github, so using those for version controlling and storing the content made sense. I know if the content is on Github then I don\'t \\"own\\" it - they can pull the plug for some reason - but let\'s not get that cynical and I\'ll probably have the repo cloned on my local.\\n\\nFor building the website, I did an internet search for `how to build blogging website using markdown files`. Couple of results showed up. Two notable were [**mdbook**](https://rust-lang.github.io/mdBook) and [**Docusaurus**](https://docusaurus.io/). I settled with Docusaurus as it provided some nice features like: tagging, read time estimations, rss feed, nicer looking UI out of the box and most importantly I didn\'t have to spend lots of time fidgeting with how to use it. Though it requires adding some metadata(`front matter`) to the markdown files, I guess I can live with it as it could be easily searched for, removed or edited if needed. I tried mdBook as well, and it felt very slick and clean for what it does and it took me under 10 minutes to set up compared to about an hour and half on Docusaurus. But it would have stitched all the blogs into a single book - which might not feel natural given that I might end up writing on bunch of stuff - which might not necessarily fit into a single book category.\\n\\nGiven that I have the content on my local or on my Github account and its just some markdown files, there are a bunch of options for publishing like using AWS S3, Github Pages, my home-server (which I don\'t run 24/7). I went with github pages(with custom domain), as it was easy to use and it doesn\'t matter - I can change the publisher whenever I want.\\n\\nWell, I guess that is it. Welcome to this blog created with **Docusaurus** and currently being served from . . . maybe check the response headers and stuff because I don\'t know where it might end up six months from now."}]}}')}}]);